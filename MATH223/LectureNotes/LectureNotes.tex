\documentclass{report} 
\usepackage{amsmath}
\usepackage{amsthm,amssymb,graphicx}
\usepackage{enumerate}
\usepackage[bookmarks]{hyperref}
\graphicspath{ {F:\repos\mcgill\MATH223\LectureNotes} }

\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

\theoremstyle{definition}
\newtheorem{_prob}{Problem}[section]
\newtheorem{_exc}{Exercise}[section]
\newtheorem*{_sol}{Solution}
\newtheorem*{_def}{Definition}
\newtheorem{_thm}{Theorem}[section]
\newtheorem{_prop}[_thm]{Proposition}
\newtheorem{_lem}[_thm]{Lemma}
\newtheorem{ex}{Example}[section]
\theoremstyle{remark}
\newtheorem{_rem}{Remark}[section]

\begin{document} 
\title{MATH 223} 
\author{Yang David Zhou}
\date{Winter 2015}
\maketitle

\section{Administrativa}

\raggedright

Professor Tiago Salvador \newline

Website: http://www.math.mcgill.ca/tsalvador/ \newline

Office: Burnside Building 1036

Office Hours: M1:45-2:45PM W2:00-3:00PM, F3:30-4:30PM \newline

\textbf{Grading}

\begin{tabular}{ l l l }
  Assignments & 15\% & 15\% \\
  Midterm     & 25\% &  0\% \\
  Final       & 60\% & 85\% \\
\end{tabular} \newline

The midterm is to be held on February 19th in class.

\chapter{Vectors}

\section[Vectors in Rn]{Vectors in \(\mathbb{R}^n\)}

\(\mathbb{R}^n\) is the set of all \(n\)-tuples of real numbers \(u=(a_1 ... a_n) \mid a\in \mathbb{R}\) where \(a\) are the \textbf{components} or \textbf{entries}.

\begin{_rem}
We use the term \textbf{scalar} to refer to an element in \(\mathbb{R}\).
\end{_rem}

\section{Basic Definitions}

\begin{_def}
\textbf{Addition}

\(u, v \in \mathbb{R}^n\)

\(u=(a_1...a_n)\) 

\(v=(b_1...b_n)\)

\(u+v=(a_1+b_1...a_n+b_n)\)
\end{_def}

\begin{_def}
\textbf{Scalar Multiplication}

\(k\in \mathbb{R}\)

\(ku=(ka_1...ka_n)\)
\end{_def}

\begin{_def}
Two vectors \(u\) and \(v\) are said to be \textbf{equal} (\(u=v\)) if \(a_i=b_i \forall i=1...n\).
\end{_def}

\begin{_def}
The \textbf{zero vector} is defined as \(0=(0...0)\).
\end{_def}

\begin{_def}
Suppose we are given \(m\) vectors \(u_1...u_m\in \mathbb{R}^n\) and \(m\) scalars \(k_1...k_m\in \mathbb{R}\).

Let \(u=k_1u_1+...+k_mu_m\).

Such a vector \(u\) is called a \textbf{linear combination} of the vectors \(u_1...u_m\).
\end{_def}

\begin{_def}
A vector \(u\) can be called a \textbf{multiple} of \(v\) if there is a scalar \(k\) such that \(u=kv\) with \(k\neq 0\). 
In the case \(k>0\) we say \(u\) is in the same direction as \(v\). 
In the case \(k<0\) we say \(u\) is in the opposite direction of \(v\).
\end{_def}

\section{The Dot Product}

\begin{_def}
Let \(u=(a_1...a_n)\) and \(v=(b_1...b_n)\). The \textbf{dot product} or inner product is given by,
\[u\cdot v=a_1b_1+...a_nb_n=\]
\end{_def}

\begin{_def}
The vectors \(u\) and \(v\) are \textbf{orthogonal} if \(u\cdot v=0\).
\end{_def}

\section{The Vector Norm}

\begin{_def}
The \textbf{norm} or \textbf{length} of a vector is given by,
\[\|u\|=\sqrt{a^2_1+...+a^2_n}\]
\end{_def}

Thus \(\|u\|\geq 0\) and \(\|u\|=0\) if and only if (iff) \(u=0\).

\begin{_def}
A vector is called a \textbf{unit vector} if \(\|u\|=1\).
\end{_def}

\begin{_def}
For any non-zero vector \(v\), the vector 
\[\hat{v}=\frac{1}{\|v\|} v\]
is the only unit vector with the same direction of $v$.
The process of finding \(\hat{v}\) is called \textbf{normalizing}.
\end{_def}

\section{Theorem: Cauchy-Schwarz Inequality}

\begin{_thm}
Given any two vectors \(u,v\in \mathbb{R}^n\), then,
\[|u\cdot v|\leq \|u\|\|v\|\]
\end{_thm}

\begin{proof}
Let \(t\in \mathbb{R}\). So, \(\|tu+v\|^2\geq 0\).
\begin{align*}
\|tu+v\|^2 &= (tu+v)(tu+v) \\
&= (tu\cdot tu)+(tu\cdot v)+(v\cdot tu)+(v\cdot v) \\
&= t^2(u\cdot u)+t(v\cdot u)+t(u\cdot v)+(v\cdot v) \\
&= t^2\|u\|^2+2t(u\cdot v)+\|v\|^2
\end{align*}
We can represent this in the form \(at^2+bt+c\geq 0\), so,
\[a=\|u\|^2, b=2(u\cdot v), c=\|v\|^2\]
Take the Discriminant as \(b^2-4ac\iff b^2\leq 4ac\).
\begin{align*}
4(u\cdot v)^2 &\leq 4\|u\|^2\|v\|^2 \\
|u\cdot v| &\leq \|u\|\|v\|
\end{align*}
\end{proof}

\section{Theorem: Minkowski Triangle Inequality}

\begin{_thm}
Given \(u,v\in \mathbb{R}^n\), then \(\|u+v\|\leq \|u\|+\|v\|\).
\end{_thm}

\begin{proof}
\begin{align*}
\|u+v\|^2 &= \|u\|^2+2(u\cdot v)+\|v\|^2 \\
&\leq \|u\|^2+2\|u\|\|v\|+\|v\|^2 \quad \text{ by C-S inequality} \\
&= (\|u\|+\|v\|)^2
\end{align*}
So, \(\|u+v\|^2\leq (\|u\|+\|v\|)^2\).
Take the square root and we are done.
\end{proof}

\section{Geometry with Vectors}

\begin{_def}
The \textbf{distance} between vectors \(u,v\in \mathbb{R}^n\) is given by,
\[d(u,v)=\|u-v\|=\sqrt{(a_1-b_1)^2+...+(a_n-b_n)^2}\]
\end{_def}

\begin{_def}
The \textbf{angle} between vectors \(u,v\in \mathbb{R}^n\) is given by,
\[cos\theta =\frac{u\cdot v}{\|u\|\|v\|} \quad \theta \in [0,\pi]\]
\end{_def}

Observe that in the previous definition, the angle is well defined.

\[-\|u\|\|v\|\leq -|u\cdot v|\leq u\cdot v\leq u\cdot v\leq |u\cdot v|\leq \|u\|\|v\|\]

Dividing the entire inequality by $\|u\|\|v\|$ yields,

\[-1\leq \frac{u\cdot v}{\|u\|\|v\|} \leq 1\]

\begin{_def}
A \textbf{hyperplane} $\mathcal{H}$ in $\mathbb{R}^n$ is the set of points $(x_1...x_n)$ that satisfy $a_1x_1+...+a_nx_n=b$ where $u=[a_1...a_n]\in \mathbb{R}^n$ and $b\in \mathbb{R}$.
\end{_def}

\begin{_def}
The \textbf{line} in $\mathbb{R}^n$ passing through a point $P=(b_1...b_n)$ and in the direction of $v\in \mathbb{R}^n$ with $v\neq 0$.
\[x=P+tu \quad t\in \mathbb{R}, \quad u=[a_1...a_n]\]
\[ \left\{ 
  \begin{array}{l}
    x_1=a_1t+b_1 \\
    x_n=a_nt+b_n
  \end{array} \right.\]
\end{_def}

\chapter{Algebra of Matrices}

\section{Introduction}

A matrix with $n$ rows and $m$ columns is written as,

\[A_{n\times m}=
\begin{bmatrix}
    a_{11} & \dots  & a_{1m} \\
    \vdots & \ddots & \vdots \\
    a_{n1} & \dots  & a_{nm}
\end{bmatrix}
\]

Or,

\[A_{n\times m}=[a_{ij}]\]

Where $a_{ij}$ is the entry in row $i$ and column $j$.

\section{Definitions and Properties of Matrices}

\begin{_def}
\textbf{Matrix Addition}
\[A+B=[a_{ij}+b_{ij}] \quad \forall i=1...n, j=1...m\]
\end{_def}

\begin{_def}
\textbf{Scalar Multiplication}
\[ka=[ka_{ij}] \quad \forall i=1...n, j=1...m\]
\end{_def}

\begin{_def}
\textbf{Zero Matrix}
\[0=[0]\]
\end{_def}

\begin{_def}
Given a matrix $A_{m\times p}$ and a matrix $B_{p\times n}$, \textbf{matrix multiplication} is defined as,
\[AB=[c_{ij}] \quad c_{ij}=\sum\limits_{k=1}^p a_{ik}b_{kj}\]
\end{_def}

\begin{_def}
Given a matrix $A$, its \textbf{transpose} is $A^T=[a_{ji}]$ where $A=[a_{ij}]$.
\end{_def}

\begin{_def}
A \textbf{square matrix} has the same number of rows as it does columns, i.e. $A_{n\times n}$ is a square matrix.
\end{_def}

\begin{_def}
Given a matrix $A=[a_{ij}]$ the elements in the \textbf{diagonal} are $[a_{11},...,a_{nn}]$.
\end{_def}

\begin{_def}
The \textbf{trace} of a matrix $A$ is given by,
\[tr(A)=a_{11}+...+a_{nn}\]
\end{_def}

\begin{_def}
The \textbf{identity matrix} $I_n$ is the matrix such that for any $n$-square matrix $A$,
\[AI=IA=A\]
\end{_def}

\begin{_def}
The \textbf{Kronecker delta} is defined by,
\[\delta_{ij}=\left\{ 
  \begin{array}{l l}
    0 & \quad \text{if } i\neq j\\
    1 & \quad \text{if } i=j
  \end{array} \right.\]
\end{_def}

\begin{_rem}
Given the definitions for the identity matrix and the Kronecker delta, an alternative definition for the identity matrix is as follows,
\[I=[\delta_{ij}]\]
\end{_rem}

\begin{_def}
A matrix $A$ is \textbf{invertible} if there is a matrix $B$ such that $AB=BA=I$.
\end{_def}

\begin{_rem}
In general, for any matrices $A$ and $B$, $AB\neq BA$.
\end{_rem}

\begin{_def}
A matrix $D$ is \textbf{diagonal} if all the non-zero entries are in the diagonal.
\[D=diaginal(d_1,...,d_n)\]
\end{_def}

\begin{_def}
A matrix $A$ is \textbf{upper triangular} if,
\[a_{ij}=0 \quad \forall i>j\]
\end{_def}

\section{Complex Numbers}

The imaginary number $i$ is defined as $i=\sqrt{-1}$ or equivalently, $i^2=-1$.

\begin{_def}
A \textbf{complex number} $z$ is given by,
\[z=a+bi \quad a,b\in \mathbb{R}\]
Where $a$ is the real part and $b$ is the imaginary part.
\end{_def}

Real numbers are also complex numbers with no imaginary component, i.e. $a+0i=a$. \newline

\textbf{Addition} for two complex numbers $z=a+bi$ and $w=c+di$ is given by,
\[z+w=(a+c)+(b+d)i\]
\textbf{Multiplication} for the same two complex numbers is given by,
\begin{align*}
z\cdot w &= (a+bi)(c+di) \\
&= ac+adi+cbi-bd \\
&= (ac-bd)+(ad+bc)i
\end{align*}

\begin{_def}
The \textbf{conjugate} of $z=a+bi$ is $\bar{z}=a-bi$.
\end{_def}

\begin{_def}
The \textbf{absolute value} or modulus of $z=a+bi$ is $|z|=\sqrt{a^2+b^2}$.
\end{_def}

\begin{ex}
\[z^{-1}=\frac{1}{z}=\frac{1}{a+bi}\cdot\frac{a-bi}{a-bi}=\frac{a-bi}{a^2+b^2}\]
\end{ex}

Observe that the following properties are true for conjugates and absolute values,
\begin{enumerate}
 \item \[z\bar{z}=|z|^2=a^2+b^2\]
 \item \[\bar{z\pm w}=\bar{z}\pm \bar{w}\]
 \item \[\bar{zw}=\bar{z}\cdot \bar{w}\]
 \item \[\bar{(\bar{z})}=z\]
 \item $z$ is real iff $z=\bar{z}$
 \item \[|zw|=|z||w|\]
 \item \[|z+w|\leq |z|+|w|\]
\end{enumerate}

\chapter{Systems of Linear Equations}

\section{Representing Linear Systems with Matrices}

Given a system of linear equations of the form,

\[\left\{ 
  \begin{array}{c}
    a_{11}x_1+...+a_{1m}x_n=b_1 \\
    \vdots \\
    a_{m1}x_1+...+a_{mn}x_n=b_m
  \end{array} \right.\]

\begin{itemize}
 \item $x_1,...,x_m$ are the unknowns, and
 \item $a_{ij}$ and $b_i$ are the constants.
\end{itemize}

The system can also be represented by matrices where,

\begin{itemize}
 \item $A=[a_{ij}]$ is the matrix of coefficients
 \item $b=[b_i]$ is the column vector of constant
 \item $M=[A|b]$ is the matrix that represents the system.
\end{itemize}

\begin{_def}
A matrix $A$ is in \textbf{echelon form} if 
\begin{enumerate}
 \item all zero rows are at the bottom, and
 \item each leading non-zero entry in a row is to the right of the leading non-zero entry in teh preceding row.
\end{enumerate}
\end{_def}

\begin{ex}
This matrix is in echelon form.
The boxed entries are the \textbf{pivots}.

\[A=
\begin{bmatrix}
 0 & \boxed{2} & 3 & 4 & 1 & 0 & 6 \\
 0 & 0 & 0 & \boxed{2} & 1 & 2 & 1 \\
 0 & 0 & 0 & 0 & 0 & \boxed{1} & 1 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
\end{ex}

\begin{_def}
A matrix is said to be in teh \textbf{row-reduced echelon form} if it is in the echelon form and,
\begin{enumerate}
 \item each pivot is equal to 1, and
 \item each pivot is the only non-zero entry in its column
\end{enumerate}
\end{_def}

\begin{ex}
This matrix is in row-reduced echelon form.
The boxed entries are the \textbf{pivots}.

\[A=
\begin{bmatrix}
 0 & \boxed{1} & 3 & 0 & 0 & 4 \\
 0 & 0 & 0 & \boxed{1} & 0 & -3 \\
 0 & 0 & 0 & 0 & \boxed{1} & 2 \\
\end{bmatrix}
\]
\end{ex}

\section{Elementary Row Operations}

Suppose that $A$ is a matrix with rows $R_1,...,R_m$.
The elementary row operations that can be performed on $A$ are as follows,

\begin{enumerate}
 \item Row interchange, $R_i\leftrightarrow R_j$
 \item Row scaling, $kR_i\rightarrow R_i$
 \item Row addition, $kR_i+R_j\rightarrow R_j$
\end{enumerate}

The method by which we find the (row-reduced) echelon form of a matrix is using the \textbf{Gaussian Elimination} algorithm. \newline

Recall that every matrix is row equivalent to a unique matrix in the row-reduced echelon form.

\begin{_def}
The \textbf{rank} of a matrix $rank(A)$ is the number of pivots in the row-reduced echelon form.
There are many other ways to define rank but they all have the same meaning.
\end{_def}

The method by which we find the inverse of a square matrix $A$ is as follows,

Let $M=[A\mid I]$. 
Find the row-reduced echelon form of $M$.
If there is a zero row in the resulting matrix then $A$ is not invertible.
Otherwise, $M\sim [I\mid B], \quad A^{-1}=B$.

\begin{_thm}
Let $A$ be a square matrix.
The following conditions are equivalent,
\begin{enumerate}
 \item $A$ is invertible
 \item the row-reduced echelon for of $A$ is $I$
 \item the only solution to $Ax=0$ is $x=0$
 \item the system $Ax=b$ has a solution for any choice of column $b$.
\end{enumerate}
\end{_thm}

A partial proof is as follows,
\begin{proof}
$(1)\Rightarrow (3)$ There is a matrix $B$ such that $AB=I=BA$.

Let $x$ be any solution of $Ax=[0]$.
\begin{align*}
BAx &= B[0] \\
Ix &= [0] \\
x &= [0]
\end{align*}

$(1)\Rightarrow (4)$ Fix a column $b$,
\begin{align*}
Ax &= b \\
\Leftrightarrow \quad A^{-1}Ax &= A^{-1}b \\
\Leftrightarrow \quad x &= A^{-1}b
\end{align*}
\end{proof}

\begin{_def}
A linear system $Ax=b$ is \textbf{homogeneous} if $b=0$.
Otherwise, $Ax=b$ is said to be \textbf{non-homogeneous}.
\end{_def}

\begin{_def}
A \textbf{particular solution} of $Ax=b$ is a vector $x$ such that $Ax=b$.
The set of all particular solutions is called the \textbf{general solution} of the solution set.
\end{_def}

\begin{_def}
A system $Ax=b$ is \textbf{consistent} if it has one or more solutions and it is said to be \textbf{inconsistent} if it has no solutions.
\end{_def}

\begin{_thm}
Any system $Ax=b$ has:

(i) an unique solution, 

(ii) no solution, or

(iii) an infinite number of solutions.
\end{_thm}

\section{Examples}

\begin{ex}
The system,
\begin{align*}
x+y+2z&=1 \\
3x-y+z&=-1 \\
-x+3y+4z&=1
\end{align*}
is equivalent to,
\[
\begin{amatrix}{3}
   1 & 1 & 2 & 1 \\
   3 & -1 & 1 & -1 \\
   -1 & 3 & 4 & 1
\end{amatrix}\sim
\begin{amatrix}{3}
   1 & 1 & 2 & 1 \\
   0 & 4 & 5 & 4 \\
   0 & 0 & 1 & -2
\end{amatrix}\]
\end{ex}

\begin{ex}
Back substitution:

$z=-2$

$4y+5z=4\Leftrightarrow y=\frac{7}{2}$

$x+y+2z=1\Leftrightarrow x=\frac{3}{2}$

\[\sim
\begin{amatrix}{3}
 1 & 0 & 0 & \frac{3}{2} \\
 0 & 1 & 0 & \frac{7}{2} \\
 0 & 0 & 1 & -2
\end{amatrix}
\]
\end{ex}

\begin{ex}
\begin{align*}
-2x+3y+3z&=-9 \\
3x-4y+z&=5 \\
-5x+7y+2z&=-14
\end{align*}

\[\sim
\begin{amatrix}{3}
 -2 & 3 & 3 & -9 \\
 3 & -4 & 1 & 5 \\
 -5 & 7 & 2 & -14
\end{amatrix}\sim
\begin{amatrix}{3}
 1 & -1 & 4 & -4 \\
 0 & 1 & 11 & -17 \\
 0 & 0 & 0 & 0
\end{amatrix}
\]

So there are infinitely many solutions.

Set $z=t$ since $z$ is a free variable, then back substitute.

\[y=-17-11t\quad x=-21-15t\quad t\in \mathbb{R}\]

So the solution space is,
\[\begin{bmatrix}
 x \\
 y \\
 z
\end{bmatrix} =
\begin{bmatrix}
 -21 \\
 -17 \\
 0
\end{bmatrix} +t
\begin{bmatrix}
 -15 \\
 -11 \\
 1
\end{bmatrix}
\]

Where $(-21, -17, 0)$ is a particular solution and $(-15, -11, 1)$ is the set of basic solutions of the homogeneous system $Ax=0$.
\end{ex}

\begin{ex}
\begin{align*}
x+2y-z&=2 \\
2x+5y-3z&=1 \\
x+4y-3z&=3
\end{align*}
\[\begin{amatrix}{3}
 1 & 2 & -7 & 2 \\
 2 & 5 & -3 & 1 \\
 1 & 4 & -3 & 3
\end{amatrix}\sim
\begin{amatrix}{3}
 1 & 2 & -1 & 2 \\
 0 & 1 & -1 & -3 \\
 0 & 0 & 0 & -1
\end{amatrix}
\]

There are no solutions possible for this system.
\end{ex}

\chapter{Vector Spaces}

\section{Introduction}

Adding two vectors in $\mathbb{R}^n$ produces a vector in $\mathbb{R}^n$.
Similarly, multiplying by a scalar produces a vector in $\mathbb{R}^n$.
These are some properties of a vector space, the following section is a formal list.

\section{Basic Definitions}

\begin{_def}

Let $V$ be a non-empty set with two operations,
\begin{enumerate}[i]
 \item Vector addition: this assigns to any $u,v\in V$ the sum $u+v\in V$.
 \item Scalar multiplication: this assigns to any $u\in V$ and $k\in K$, a product $ku\in V$ where $k$ is a field.
\end{enumerate}
Then $V$ is called a \textbf{vector} space (over the field $K$) if the following axioms hold for any $u,v,w\in V$.
\begin{itemize}
 \item A1) $(u+v)+w=u+(v+w)$
 \item A2) there is a vector in $V$ denoted by $0$ called the zero vector such that $v+0=v$ for any $v\in V$.
 \item A3) for each $u\in V$, there is a vector in $V$ denoted $-u$, such that $u+(-u)=0$.
 $-u$ is called the negative of $u$.
 \item A4) $u+v=v+u$
 \item A5) $k(u+v)=ku+kv$ for any scalar $k\in K$
 \item A6) $(a+b)u=au+bu$ for any scalars $a,b\in K$
 \item A7) $(ab)u=a(bu)$ for any scalars $a,b\in K$
 \item A8) $1u=u$ for the unit scalar $k\in K$
\end{itemize}
\end{_def}

\begin{_rem}
A \textbf{field} $K$ is a mathematical object with nice properties, with $\mathbb{R}$ and $\mathbb{C}$ being two examples. From now on, we will take it to be $\mathbb{R}$ or $\mathbb{C}$.
\end{_rem}

\section{Examples of Vector Spaces}

\begin{ex}
These are some examples of vector spaces,
\begin{enumerate}
 \item $\mathbb{R}^n$
 \item $\mathbb{C}^n$
 \item The matrix space: $M_{m\times n}$
 
 $M_{m\times n}$ denotes the set of all matrices with size $m$ rows, $n$ columns and real entries.
 $M_{m\times n}(\mathbb{C})$ permits the entries to be complex.
 The space of the real matrices are a subset of the space of complex matrices.
 \item The polynomial space: $P(t)$
 
 $P(t)$ denotes the set of all polynomials of the form,
 \[P(t)=a_0+a_1t+...+a_nt^n \mid a_i\in \mathbb{R}\]
 \item The function space: $F(x)$
 
 Let $X$ be a non-empty set.
 Let $F(x)$ denote the set of all functions of $X$ into $\mathbb{R}$.
 Then $F(x)$ is a vector space (over $\mathbb{R}$) with respect to the following operations,
 \begin{enumerate}[i]
  \item vector addition: 
  \[(f+g)(x)=f(x)+g(x)\mid \forall x\in X\]
  \item scalar multiplication: for any $k\in K, f\in F(x)$
  \[(kf)(x)=kf(x) \mid \forall x\in X\]
  \item zero function: $\underline{0}(x)=0$ 
 \end{enumerate}
\end{enumerate}
\end{ex}

\begin{_exc}
Consider the set $\mathbb{R}^2$ with the usual scalar multiplication, but with the following vector addition:
\[(a,b)\diamond (c,d)=(a+d,b+c)\]

Is this a vector space?

No because axiom 4 does not hold.

\[(1,2)\diamond (-1,1)=(2,1)\]
\[(-1,1)\diamond (1,2)=(1,2)\]
\end{_exc}

\section{Vector Subspaces}

\begin{_def}
Let $V$ be a vector space and $W$ be a subset of $V$.
Then $W$ is a \textbf{subspace} of $V$ if $W$ itself is a vector space with the operations of vector addition and scalar multiplication of $V$.
\end{_def}

\begin{ex}
$P(t)$ is a subspace of $F(\mathbb{R})$
\end{ex}

The next theorem provides a simple criteria to show that a subset $W$ of $V$ is a subspace.

\begin{_thm}
Suppose that $W$ is a subset of $V$, with $V$ being a vector space. Then $W$ is a subspace if the following two conditions hold:
\begin{enumerate}[i]
 \item The zero vector $0$ belongs to $W$.
 \item For every two vectors $u,v\in W$ and $k\in R$
 \begin{itemize}
  \item $u+v\in W$ (closed under vector addition)
  \item $ku\in W$ (closed under scalar multiplication)
 \end{itemize}
\end{enumerate}
\end{_thm}

\begin{proof}
By (i), $W$ is non-empty.

By (ii), the operations of vector addition and scalar multiplication are well defined.

The it remains to prove each of the axioms of a vector space.

A1, 4, 5, 6, 7, and 8 hold in $W$ because they hold in $V$.

A2 is true by (i).

A3: Let $v\in W$.
We know that $-v\in V$ with $v+(-v)=0$ by A3 for the vector space $V$.
But $W$ is closed under scalar multiplication (by (ii)) and so $v\in W$ and we are done.
\end{proof}

\section{Examples of Vector Subspaces}

\begin{ex}
These are some examples of vector subspaces,
\begin{enumerate}
 \item ${0}, V$ are subspaces of $V$.
 These are called the trivial subspaces of $V$.
 \item Subspaces of $\mathbb{R}^3$
 \begin{enumerate}[i]
  \item Line through the origin is a subspace.
  \item Planes through the origin.
 \end{enumerate}
 \item Subspaces of $P(t)$
 \begin{enumerate}[i]
  \item $P_m(t)=\{p(\cdot)\in P(t); degree(p(\cdot))\leq m\}$
  \item $Q(t)$ is the set of polynomials with only even powers
 \end{enumerate}
 \item Subspaces of matrices $M_{m\times n}$
 \begin{enumerate}[i]
  \item $W_1=\{A\in M_{m\times n}; A \text{ is diagonal}\}$
  \item $W_2=\{A\in M+{m\times n}; A=A^T\}$
 \end{enumerate}
 \item Subspaces of $F(\mathbb{R})$
 \begin{enumerate}[i]
  \item $C(\mathbb{R})=\{f\in F(\mathbb{R}); f \text{ is continuous}\}$
  \item $C'(\mathbb{R})=\{f\in F(\mathbb{R}); f \text{ is differentiable}\}$
 \end{enumerate}
\end{enumerate}
\end{ex}

\section{More on Vector Spaces}

\begin{_def}
Let $A\in M_{m\times n}$. The nullspace of $A$ is $N(A)$ which is given by,
\[N(A)=\{x\in \mathbb{R}^n \mid Ax=0\}\]
\end{_def}

\begin{_prop}
$N(A)$ is a subspace of $\mathbb{R}^n$
\end{_prop}
\begin{proof}
Clearly $N(A)$ is a subset of $\mathbb{R}^n$
\begin{enumerate}[i]
 \item $0\in N(A)$. True because $A0=0$.
 \item Let $u,v\in N(A)$, and $a,b\in \mathbb{R}$.
 We want to show that $au+bv\in N(A)$, which is the same as $A(au+bv)=0$
 \begin{align*}
 A(au+bv) &= A(au)+a(bv) \\
 &= a(Au)+b(Av) \quad \text{Since } u,v\in N(A) \text{, then } Au=0, Av=0 \\
 &= a0+b0 \\
 &= 0
 \end{align*}
\end{enumerate}
\end{proof}

\begin{_rem}
The solution set of a non-homogeneous system $\{x\in \mathbb{R}^n \mid Ax=b\}$ where $b\neq 0$ is not a subspace because the zero vector is not present.
\end{_rem}

\begin{_thm}
Let $U$ and $W$ be subspaces of a vector space $V$.
Then $U\cap W$ is also a subspace.
\end{_thm}

\begin{proof}
Since $U\subseteq V$ and $W\subseteq V$ ($U$ and $W$ are subspaces),
\[U\cap W\subseteq V\]
\begin{enumerate}[i]
 \item So, $0\in V$ and $0\in W$, therefore $0\in U\cap W$
 \item Let $u,v\in U\cap W$ and $a,b\in \mathbb{R}$
 \begin{align*}
  u,v\in U\cap W &\Rightarrow \left\{ 
  \begin{array}{l}
    u,v\in V \\
    u,v\in W
  \end{array} \right. \\
  &\Rightarrow \left\{ 
  \begin{array}{l}
    au+bv\in V \\
    au+bv\in W
  \end{array} \right. \quad \text{both } U \text{ and } W \text{are subspaces} \\
  &\Rightarrow au+bv\in U \cap W
 \end{align*}
\end{enumerate}
\end{proof}

\begin{_rem}
In general, if $U$ and $W$ are subspaces, $U\cup W$ is \textbf{not} a subspace.
An example would be two lines through the origin in $\mathbb{R}^3$.
\end{_rem}

\section{Linear Combinations}

Observe that $au+bv$ is a linear combination.

\begin{_def}
Let $U$ be a vector space.
A vector $v\in V$ is a \textbf{linear combination} of $u_1,...,u_m$ in $V$ if there exists scalars $a_1,...,a_m$ so that,
\[v=a_1u_1+...+a_mu_m\]
\end{_def}

\begin{ex}
The following is an example of linear combinations in $\mathbb{R}^3$.

Is $v=(1,5,5)\in \mathbb{R}^3$ a linear combination of $u_1=(1,2,3)$, $u_2=(1,0,1)$, $u_3=(0,1,0)$?

That is the same as asking, are there constants $a, b, c\in \mathbb{R}$ such that,
\[v=au_1+bu_2+cu_3\]

That is, are there $a,b,c\in \mathbb{R}$ such that,

\[\begin{bmatrix}
1 \\ 5 \\ 5
\end{bmatrix}=a\begin{bmatrix}
1 \\ 2 \\ 3
\end{bmatrix}+b\begin{bmatrix}
1 \\ 0 \\ 1
\end{bmatrix}+c\begin{bmatrix}
0 \\ 1 \\ 0
\end{bmatrix}\]

Yes. Take $a=2, b=-1, c=1$.
\end{ex}

\begin{_def}
Let $A\in M_{m\times n}$.
The column space of $A$ is $C(A)$ which consists of all linear combinations of the columns of $A$.
Alternatively,
\[C(A)=\{Ax\mid x\in \mathbb{R}^n\}\]
\end{_def}

\begin{_prop}
The linear system $Ax=b$ is consistent iff $b\in C(A)$.
\end{_prop}

\begin{ex}
The following is an example of linear combinations in $P(t)$.

Is the polynomial $P(t)=t^2+5t+5$ a linear combination of the polynomials $P_1(t)=t^2+2t+3, P_2(t)=t^2+1, P_3(t)=t$?

Equivalently, are there scalars $a,b,c\in \mathbb{R}$ such that,
\[p(\cdot)=ap_1(\cdot)+bp_2(\cdot)+cp_3(\cdot)\]

There are two ways of solving this,
\begin{enumerate}
 \item Matching coefficients:
 \[t^2+5t+5=(a+b)t^2+(2a+c)t+3a+b\]
 \[\left\{ 
  \begin{array}{l}
    1 = a+b \\
    5=2a+c \\
    5=3a+b
  \end{array} \right. \Leftrightarrow
  \begin{bmatrix}
  1 & 1 & 0 \\
  2 & 0 & 1 \\
  3 & 1 & 0
  \end{bmatrix}
  \begin{bmatrix}
  a \\ b \\ c
  \end{bmatrix}=
  \begin{bmatrix}
  1 \\ 5 \\ 5
  \end{bmatrix}\]
 \item "Trial approach":
 We set $t$ in $P(t)$ equal to the three distinct values and each one provides a different equation,
 \[\begin{array}{l l}
 t=0 & 5=3a+b \\
 t=1 & 11=6a+2b+c \\
 t=-1 & 1=2a+2b-c
 \end{array}\]
 Then solve for $a, b, c$.
\end{enumerate}
\end{ex}

\begin{ex} The following are two examples of subspaces of $\mathbb{R}^3$
\begin{enumerate}
 \item A line with direction $(1,2,3)$ through the origin,
 \[\{t\begin{bmatrix}
 1 \\ 2 \\ 3
 \end{bmatrix} \mid t\in \mathbb{R} \}\]
 \item A plane through the origin,
 \begin{align*}
 \{(x,y,z)\in \mathbb{R}^3\mid z=0\}
 &=\{t(1,0,0)+s(0,1,0)\mid t,s\in \mathbb{R}\} \\
 &=\{(t,s,0)\mid t,s\in \mathbb{R}\}
 \end{align*}

\end{enumerate}
\end{ex}

\section{The Span of a Vector Space}

\begin{_def}
Let $u_1,...,u_m$ be vectors in $V$.
The set of all linear combinations of $u_1,...,u_m$ is called the \textbf{span} of $u_1,...,u_m$ and is denoted by $span\{u_1,...,u_m\}$.
\[span\{u_1,...,u_m\}=\{t_1u_1+...+t_mu_m\mid t_1,...,t_m\in \mathbb{R}\}\]
\end{_def}

\begin{_def}
The vectors $u_1,...,u_m\in V$ are said to span $V$ or to form a \textbf{spanning set} of $V$ if,
\[span\{u_1,...,u_m\}=V\]
\end{_def}

The following are the properties of spans.
\begin{itemize}
 \item If $span\{u_1,...,u_m\}=V$, then for any $v\in V$, $span\{v,u_1,...,u_m\}=V$.
 \item If $span\{0,u_1,...,u_m\}=V$, then $span\{u_1,...,u_m\}=V$.
 \item If $span\{u_1,...,u_m\}=V$ and $u_k$ is a linear combination of $u_1,...,u_{k-1},u_{k+1},...,u_m$ then $span\{u_1,...,u_{k-1},u_{k+1},...,u_m\}=V$.
\end{itemize}

\begin{_prop}
Let $u_1,...,u_m$ be vectors in $V$.
Then $span\{u_1,...,u_m\}$ is a subspace.
\end{_prop}

\begin{proof}
Clearly $span\{u_1,...,u_m\}\subseteq V$.

We know $0\in span\{u_1,...,u_m\}$ since,
\[0=0u_1+...+0u_m\in span\{u_1,...,u_m\}\]
Take any $u,v\in span\{u_1,...,u_m\}$ and $a,b\in \mathbb{R}$.
\[u=a_1u_1+...+a_mu_m \quad a_1,...,a_m\in\mathbb{R}\text{ since } u\in span\{u_1,...,u_m\}\]
Likewise,
\[v=b_1u_1+...+b_mu_m \quad b_1,...,b_m\in\mathbb{R}\]
So,
\begin{align*}
au+bv&=aa_1u_1+...+aa_mu_m+bb_1u_1+...+bb_mu_m \\
&=(aa_1+bb_1)u_1+...+(aa_m+bb_m)u_m
\end{align*}
Which shows that $au+bv$ is a linear combination of $u_1,....u_m$ with scalars $aa_1+bb_1,...,aa_m+bb_m$.
\end{proof}

\begin{_exc}
$W=\{(x,y,z)\in \mathbb{R}^3\mid x=2y=3z\}$

Clearly, $W\subseteq \mathbb{R}^3$

$(0,0,0)\in W$ is true because $0=2(0)=3(0)$.

$u,v\in W\quad a,b\in \mathbb{R}$

$u=(u_1,u_2,u_3)$ with (1) $u_1=2u_2$ and (2) $2u_2=3u_3$

$v=(v_1,v_2,v_3)$ with (3) $v_1=2v_2$ and (4) $2v_2=3v_3$

\begin{align*}
z=(z_1,z_2,z_3)&=au+bv \\
&=(au_1+bv_1,au_2+bv_2,au_3+bv_3)
\end{align*}

We want to show $z\in W$, so,
\begin{align*}
z_1&=2z_2=3z_3 \\
z_1&=au_1+bv_1 \\
&=a(2u_2)+b(2v_2) \quad \text{by (1) and (3)}\\
&=2(au_2+bv_2) \\
&=2z_2 \\
&=a(3u_3)+b(3v_3) \\
&=3(au_3+bv_3) \\
&=3z_3
\end{align*}
So, $z_1=2z_2=3z_3$.

It is also a line through the origin,
\[x=2y=3z\Rightarrow\]
\[\{t\begin{bmatrix}
6 \\ 3 \\ 2
\end{bmatrix}\mid t\in \mathbb{R}\}=span\{(6,3,2)\}\]
\end{_exc}

\begin{_def}
The \textbf{span of a set} $S$ is the set of all linear combinations of vectors in $S$.
If $S\neq 0$, $span(S)=\{0\}$.
\end{_def}

\begin{_thm}
Let $S$ be a subset of the vector space $V$.
Then,
\begin{enumerate}[i)]
 \item $span(S)$ is a subspace of $V$.
 \item if $W$ is a subspace of $V$ such that $S\subseteq W$, then $span(S)\subseteq W$.
\end{enumerate}
\end{_thm}

*Proof here*

\section{The Row Space of a Matrix}

\begin{_def}
Let $A\in M_{m\times n}$.
The \textbf{row space} of $A$, written as $rowsp(A)$, is the set of all linear combinations of rows of $A$.
\[rowsp(A)=col(A^T)\]
The notation for column space can also be $colsp(A^T)$.
$A\in M_{m\times n}$.
$rowsp(A)$ is a subspace of $\mathbb{R}^n$.
$col(A)$ is a subspace of $\mathbb{R}^m$.
\end{_def}

\begin{ex}
Two matrices are row equivalent if you can get from one to the other with only elementary row operations.
\[A=
\begin{bmatrix}
1 & 2 & -1 & 3 \\
2 & 4 & -1 & 5 \\
3 & 6 & -2 & 8
\end{bmatrix}\sim
\begin{bmatrix}
1 & 2 & -1 & 3 \\
0 & 0 & 1 & -1 \\
0 & 0 & 0 & 0
\end{bmatrix}=B
\]
The following is true for $A$ and $B$,
\begin{itemize}
 \item $rowsp(A)=rowsp(B)$
 \item $rowsp(A)=span{(1,2,-1,3),(2,4,-1,5),(3,6,-2,8)}$
 \item $rowsp(B)=span{(1,2,-1,3),(0,0,1,-1)}$
\end{itemize}
Observe that any basis of a subspace is not unique.
\end{ex}

\begin{_thm}
Row equivalent matrices have the same row space.
\end{_thm}

\section{Linear Dependence and Independence}

\begin{_def}
The vectors $v_1,...,v_n$ are \textbf{linearly independent} if the following condition is satisfied,
\[\text{if }a_1v_1+...+a_nv_n=0\text{ , then }a_1=...=a_n=0\]
The vectors $v_1,...,v_n$ are \textbf{linearly dependent} if they are not linearly independent.
\end{_def}

\begin{_rem}
Consider the vector equation,
\[x_1v_1+...+x_nv_n=0\]
where $x_1,...,x_n$ are unknown scalars.
If the only solution is $(0,..,0)$, then the vectors are linearly independent.
Otherwise they are linearly dependent.
\end{_rem}

\begin{ex}
The following is an example of linear dependence in $\mathbb{R}^3$.

Geometrically, linearly dependent vectors run in the same direction.
\begin{enumerate}[i)]
 \item Two vectors in $\mathbb{R}^3$ are linearly dependent if they lie on the same line.
 i.e., $k\in \mathbb{R}, k\neq 0$
 \[v_2=kv_1 \Leftrightarrow kv_1-v_2=0\]
 \item Three vectors in $\mathbb{R}^3$ are linearly dependent if they lie on the same plane.
 i.e., $a_1,a_2\in \mathbb{R}, a_1,a_2\neq 0$
 \[v_3=a_1v_1+a_2v_2 \Leftrightarrow a_1v_1+a_2v_2-v_3=0\]
\end{enumerate}
\end{ex}

\begin{_def}
An \textbf{infinite set of vectors} $S$ is linearly dependent if there exist vectors $v_1,...,v_n\in S$ that are linearly dependent.
\end{_def}

\begin{_prop}
Let $V$ be a vector space.
\begin{enumerate}[i)]
 \item If $v\neq 0, \{v\}$ is linearly independent.
 \item No independent set of vectors contains the zero vector.
 Any non-zero scalar multiplied by the zero vector will still yield the zero vector.
 \item \underline{Two} vectors are linearly dependent iff one of them is a multiple of the others.
 Let $v_1, v_2$ be linearly dependent and $a_1\neq 0$.
 \begin{align*}
 &a_1v_1+a_2v_2=0 \\
 \Rightarrow \quad & v_1+\frac{a_2}{a_1}v_2=0 \\
 \Leftrightarrow \quad & v_1=\frac{-a_2}{a_1}v_2
 \end{align*}
 \item No independent set can contain two vectors that are multiples of each other.
\end{enumerate}
\end{_prop}

\begin{_exc}
Show that ${1+t,3t+t^2,2+t-t^2}$ is linearly independent in $P_2(t)$.

Suppose,
\[a(1+t)+b(3t+t^2)+c(2+t-t^2)=0\quad \forall t\in \mathbb{R}\]
We want to show $a=b=c=0$.

Substitute three different values for $t$ to obtain three equations, then solve.
\begin{align*}
t=0\quad & a+b(0)+2(c)=0\Leftrightarrow a+2c=0\\
t=-1\quad & -2b=0\Leftrightarrow b=0\\
t=1\quad & 2a+0+2c=0\Leftrightarrow 2a+2c=0\quad \text{zero term from }t=-1
\end{align*}
So, $a+2c=0$ (1) and $2a+2c=0$ (2).
(2)$-$(1) gives $a=0$.

And we are done.

The alternate method is to match coefficients and solve that system as in the example in 4.7.
\end{_exc}

\begin{_prop}
The vectors $v_1,...,v_n$ are linearly dependent iff one of them is a linear combination of another.
\end{_prop}

\begin{proof}
($\Rightarrow$)

The vectors are linearly dependent.
Then, $\forall a_1,...,a_n \exists a_i\neq 0$ such that $a_1v_1+...+a_nv_n=0$.

Say $a_k\neq 0$.
\begin{align*}
& a_1v_1+...+a_nv_n=0 \\
\Leftrightarrow \quad &\frac{a_1}{a_k}v_1+...+\frac{a_{k-1}}{a_k}+v_{k-1}+v_k+\frac{a_{k+1}}{a_k}v_{k+1}+...+\frac{a_n}{a_k}v_n \\
\Leftrightarrow \quad &v_k=-\frac{a_1}{a_k}v_1-...-\frac{a_{k-1}}{a_k}-v_{k-1}-\frac{a_{k+1}}{a_k}v_{k+1}-...-\frac{a_n}{a_k}v_n
\end{align*}
Observe that $v_k$ can be any vector including the zero vector.

($\Leftarrow$)

Say $v_i$ is a linear combination of the other vectors.
Then,
\begin{align*}
& v_i=b_1v_1+...+b_{i-1}v_{i-1}+b_{i+1}v_{i+1}+...+b_nv_n \\
\Leftrightarrow \quad & 0=b_1v_1+...+b_{i-1}v_{i-1}-v_i+b_{i+1}v_{i+1}+...+b_nv_n
\end{align*}
So, the scalar on $v_i$ is $-1$ which is not zero.

And we are done.
\end{proof}

\section{Basis}

\begin{_def}
A set $B=\{u_1,...,u_n\}$ of vectors in $V$ is a \textbf{basis} of $V$ if two conditions are satisfied,
\begin{enumerate}[i)]
 \item $B$ is a linearly independent set
 \item $span(B)=V$
\end{enumerate}
\end{_def}

\begin{ex}
The following are examples of basis,
\begin{enumerate}[1)]
 \item $\mathbb{R}^n$
 \[e_1=(1,0,...,0), e_2=(0,1,0,...,0), e_n=(0,...,0,1)\]
 $\{e_1,...,e_n\}$ is a basis for $\mathbb{R}^n$.

 $(1,2,3)=e_1+2e_2+3e_3$. This is the canonical or standard basis.
 \item $P_m(t)$
 \[\{1,t,t^2,...,t^m\}\]
 \item $M_{m\times n}$
 For $M_{2\times 3}$ it is,
 \[
 \begin{bmatrix}
 1&0&0\\0&0&0
 \end{bmatrix},
 \begin{bmatrix}
 0&1&0\\0&0&0
 \end{bmatrix},
 \begin{bmatrix}
 0&0&1\\0&0&0
 \end{bmatrix}\]
 \[
 \begin{bmatrix}
 0&0&0\\1&0&0
 \end{bmatrix},
 \begin{bmatrix}
 0&0&0\\0&1&0
 \end{bmatrix},
 \begin{bmatrix}
 0&0&0\\0&0&1
 \end{bmatrix}\]
\end{enumerate}
\end{ex}

Observe that a basis can also be the minimum span of a vector space.

\begin{ex}
\[A=
\begin{bmatrix}
1 & 2 & 0 & 1 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{bmatrix}
\]

$\{R_1, R_2, R_3\}$ is linearly independent as none of the rows are linear combinations of the others.

Observe that $R_1, R_2, R_3$ spans the rowspace of $A$.
\end{ex}

\begin{_thm}
The non-zero rows of a matrix in echelon form are linearly independent and form a basis for the row space.
\end{_thm}

\begin{_prop}
A set $B=\{v_1,...,v_n\}$ is a basis of $V$ iff every vector $v\in V$ can be uniquely written as a linear combination of $v_1,...,v_n$.
\end{_prop}

\begin{proof} ($\Rightarrow$)

Suppose $v=a_1v_1+...+a_nv_n\quad a_1,...,a_n\in \mathbb{R}$ and $v=b_1v_1+...+b_nv_n\quad b_1,...,b_n\in \mathbb{R}$.

We want to show that $a_i=b_i\quad \forall i=1...n$.

We have,
\begin{align*}
& a_1v_1+...+a_nv_n=b_1v_1+...+b_nv_n \\
\Leftrightarrow \quad & 0=(a_1-b_1)v_1+...+(a_n-b_n)v_n
\end{align*}

Since $\{v_1,...,v_n\}$ is linearly independent,
\begin{align*}
& a_1-b_1=0,...,a_n-b_n=0 \\
\Leftrightarrow \quad & a_1=b_1,...,a_n=b_n
\end{align*}

($\Leftarrow$)

Suppose $a_1v_1+...+a_nv_n=0$. 
We have $a_1,...,a_n\in \mathbb{R}$.
To show that $B$ is linearly independent, we must show $a_1=...=a_n=0$,
\[0=0(v_1)+...+0(v_n)\]

We assumed the linear combination is unique,
\[a_1=...=a_n=0\]

We assumed every vector can be written as a linear combination.
$B$ spans $V$ by the assumption.

And we are done.
\end{proof}

\section{Coordinates}

\begin{_def}
Let $V$ be a vector space and $B=\{v_1,...,v_n\}$ a basis of $V$.
Then for any $v\in V$,
\[v=a_1v_1+...+a_nv_n\]
where the $a_1,...,a_n$ are unique for $v$.

We call these scalars the \textbf{coordinates} of $v$ in the basis $B$ and they form a vector $[a_1,...,a_n]$ called coordinate vectors of $v$ relative to $B$ and denoted by $[v]_B$.
\end{_def}

\begin{ex}
${t+1, t-1, (t-1)^2}$ form a basis of $P_2(t)$.
This can be written as :
\[P(t)=(t+1)-(t-1)+(t-1)^2=t^2-2+2\]
or as coordinates: $[P(\cdot)]_B=[1,-1,1]$.
\end{ex}

\section{Dimension}

Next we will give a series of auxiliary lemmas and propositions to show that the size of any two basis for a vector space has the same number of vectors.

\begin{_prop}
Let $V$ be a vector space and $S=\{v_1,...,v_n\}$ be a spanning set of $V$.
Then,
\begin{enumerate}[i)]
 \item if $w\in V$, $\{w,v_1,...,v_n\}$ is linearly dependent and spans $V$.
 \item if $v_i$ is a linear combination of the other vectors, $S$ without $v_i$ still spans $V$.
\end{enumerate}
\end{_prop}

\begin{_lem}
Suppose $\{v_1,...,v_k\}$ is linearly dependent and all the vectors are non-zero.
Then one of the vectors is a linear combination of the preceding vectors.
\end{_lem}
\begin{proof}
Then there are scalars, not all zero such that,
\[a_1v_1+...+a_nv_n=0 \quad \text{where } a_1,...,a_k\in\mathbb{R}\]

Let $i$ be the largest index such that $a_i\neq 0$.
We claim $i>1$.
If $i=1$, then $a_1v_1=0$, which is a contradiction since $v_1 \neq 0$.
\begin{align*}
\Rightarrow \quad & a_1v_1+...+a_iv_i=0 \\
\Leftrightarrow \quad & v_i=-\frac{a_1}{a_2}-...-\frac{a_{i-1}}{a_i}v_{i-1}
\end{align*}
\end{proof}

\begin{_lem} \underline{"Replacement Lemma".}

Suppose $\{v_1,...,v_n\}$ spans $V$ and $\{w_1,...,w_m\}$ is linearly independent.
Then $m\leq n$ and $V$ is spanned by a set of the form ,
\[\{w_1,...,w_m,v_{i_1},...,v_{i_{n-m}}\}\]
Thus any $n+1$ or more vectors in $V$ are linearly dependent.
\end{_lem}

\begin{proof} (General idea)
In $span\{v_1,...,v_n\}=V$, we add $w_1$.
$span\{w_1,v_1,...,v_n\}=V$ but it is now linearly dependent.
Assume $v_1,...,v_n$ are all non-zero.

By proposition and lemma from above, we can remove $v_i$ so that
\[span\{w_1,v_1,...,v_{i-1},v_{i+1},...,v_n\}=V\]
Repeat the above steps when adding $w_2$ and removing $v_k$ and we get,
\[span\{w_1,w_2,v_1,...,v_{k-1},v_{k+1},...,v_{i-1},v_{i+1},...,v_n\}=V\]

If $m\leq n$,
\[span\{w_1,...,w_m,v_{i_1},...,v_{i_{n-m}}\}=V\]

Suppose $m>n$,
\[span\{w_1,...,w_n\}=V\]
If we add $w_{n+1}$, then $span\{w_1,...,w_n,w_{n+1}\}=V$ and is linearly dependent.
But this is a contradiction, $w_1,...,w_m$ were all linearly independent.
The only possible case is then $m\leq n$.
\end{proof}

\begin{_rem}
The Replacement Lemma tells us that the size of any spanning set is at least as big as the size of any linearly independent set.
\end{_rem}

The preceding lemmas and propositions in this section were auxiliary to proving the following theorem.

\begin{_thm}
Let $\{u_1,...,u_m\}$ and $\{v_1,...,v_m\}$ be basis for $V$.
Then $m=n$.
\end{_thm}

\begin{proof}
The proof is relatively simple as we have the Replacement Lemma.

$\{u_1,...,u_m\}$ is (1) linearly independent and (2) spans $V$.

$\{v_1,...,v_n\}$ (3) spans $V$ and (4) is linearly independent.

Apply the remark on the Replacement Lemma twice to statements (1) with (3) and (2) with (4).
Since, $m\leq n$ and $m\geq n$, then $n=m$.
\end{proof}

\begin{_def}
A vector space is said to be of \textbf{finite dimension} or \textbf{n-dimensional}, written $dim(V)=n$ if $V$ has a basis with $n$ elements.
The vector space $\{0\}$ has dimension $0$.
If a vector $V$ does not have a finite basis, then $V$ is said to be of infinite dimension or infinite dimensional.
\end{_def}

\begin{ex} The following are examples of dimensions,

\begin{enumerate}[1)]
 \item $dim(\mathbb{R}^n)=n$
 \item $dim(M_{m\times n})=mn$
 \item $dim(P_n(t))=n+1 \quad \text{because of }(t^0,t^1,...,t^n)$
 \item $dim(P(t))=\infty$
\end{enumerate}
\end{ex}

To find the basis of a set of vectors, put the vectors into matrix rows and find the echelon form.

\begin{_thm}
Let $V$ be a vector space of dimension $n$.
Then,
\begin{enumerate}[i)]
 \item any $n+1$ or more vectors are linearly dependent, and
 \item any linearly independent set $S=\{u_1,...,u_n\}$ with $n$ elements is a basis of $V$, and
 \item any spanning set $\{v_1,...,v_n\}$ is a basis of $V$.
\end{enumerate}
\end{_thm}

\begin{_prop}
Let $V$ be a vector space and $\{v_1,...,v_k\}$ a linearly independent set.
Suppose that $w\in V$ with $w\notin span\{v_1,...,v_k\}$, then $\{w,v_1,...,v_k\}$ is linearly independent.
\end{_prop}

\begin{_thm}
\underline{"Basis Extension"}

Let $V$ be a vector space of dimension $m$ and a set $\{v_1,...,v_k\}$ of vectors in $V$ that is linearly independent and $k<n$.
Then there exist $n-k$ vectors $w_1,...,w_{n-k}$ such that $\{v_1,...,v_k,w_1,...,w_{n-k}\}$ is a basis of $V$.
\end{_thm}

\begin{proof}
$\{v_1,...,v_k\}$ is not a basis since $k<n$ and basis of $V$ have $n$ elements.
So, $\{v_1,...,v_k\}$ does not span $V$.

Then there is $w_1\in V$ such that $w_1\notin span\{v_1,...,v_k\}$.

By the proposition, $\{w_1,v_1,...,v_k\}$ is linearly independent.

If $k+1=n$, we are done.

If $k+1<n$, we repeat the argument $n-k$ times to get a set $\{v_1,...,v_k,w_1,...,w_{n-k}\}$ that is linearly independent.
This set has $n$ elements and is linearly independent, so part (ii) of the above theorem guarantees that it is a basis.
\end{proof}

\begin{_thm}
Let $W$ be a subspace of an $n$-dimensional vector space $V$.
Then,
\[dim(W)\leq n\]
\end{_thm}

\begin{proof}
Suppose we have the basis of $W$ with $k$ elements.
By the replacement lemma, $k\leq n$.
\end{proof}

\section{Application to Matrices}

\begin{_exc}
Find a basis for the row space, column space, and null space of the matrix $A$.

\[A=\begin{bmatrix}
1 & 2 & 3 & 4 & 5 \\
2 & 3 & 7 & 7 & 8 \\
1 & 3 & 2 & 4 & 6 \\
1 & 2 & 3 & 3 & 4
\end{bmatrix}\]

Row space:

Put the matrix into row-reduced form.

\[A\sim\begin{bmatrix}
1 & 2 & 3 & 4 & 5 \\
0 & 1 & -1 & -2 & -2 \\
0 & 0 & 0 & -1 & 1 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}=M\]

\begin{align*}
rowsp(A)&=rowsp(M) \\
&=span\{(1,2,3,4,5),(0,1,-1,-1,2),(0,0,0,-1,1)\}
\end{align*}

Therefore $\{(1,2,3,4,5),(0,1,-1,-1,2),(0,0,0,-1,1)\}$ is a basis of $rowsp(A)$.
\[dim(rowsp(A))=3\]

Column space:

\[col(A)=span\{(1,2,1,1),(2,3,3,2),(4,7,4,3)\}\]

\[M=\begin{bmatrix}
\boxed{1} & 2 & 3 & 4 & 5 \\
0 & \boxed{1} & -1 & -2 & -2 \\
0 & 0 & 0 & \boxed{-1} & 1 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}\]

The boxed entries (the pivots) are the columns of the column space in the original matrix.

Otherwise, we must check whether every column vector in $col(A)$ can be added to the basis (i.e. are linear combinations of the preceding column vectors.
\[dim(col(A))=3\]

Null space:

\[\begin{amatrix}{5}
1 & 2 & 3 & 4 & 5 & 0 \\
0 & 1 &-1 &-1 &-2 & 0 \\
0 & 0 & 0 &-1 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0
\end{amatrix}\]

\[x_3=t\quad x_5=s\]
\begin{align*}
0&=-x_4+x_5\\
0&=x_2-x_3-x_4-2x_5\\
0&=x_1+2x_2+3x_3+4x_4+5x_5
\end{align*}

\[\begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
\end{bmatrix}=t
\begin{bmatrix}
-5 \\ 1 \\ 1 \\ 0 \\ 0
\end{bmatrix}+s
\begin{bmatrix}
3 \\ 1 \\ 0 \\ -1 \\ 1
\end{bmatrix}\]

\[N(A)=span\{(-5,1,1,0,0),(3,1,0,-1,1)\}\]

So, $\{(-5,1,1,0,0),(3,1,0,-1,1)\}$ is a basis for $N(A)$.
\[dim(N(A))=2\]
\end{_exc}

\begin{_thm}
\underline{"Fundamental Theorem of Linear Algebra, part I}

Let $A\in M_{m\times n}$ with $rank(A)=r$.
Then the row space and the column space both have dimension $r$ and the null space has dimension $m-r$.
\end{_thm}

\section{Sums and Direct Sums}

\begin{_def}
Let $U$ and $W$ be subspaces of a vector space $V$.
The \textbf{sum} of $V$ and $W$ is given by,
\[U+W=\{v\in V \mid v=u+w, u\in U, w\in W\}\]
\end{_def}

Vector space sum is not the same as set union.

\begin{ex}
The following are examples of the sum of subspaces of $M_{2\times 2}$.

\[U=\left\lbrace
\begin{bmatrix}
a & b \\ 0 & 0
\end{bmatrix}
\mid
a, b\in \mathbb{R}
\right\rbrace
\quad
dim(U)=2
\]

\[W=\left\lbrace
\begin{bmatrix}
a & 0 \\ c & 0
\end{bmatrix}
\mid
a, c\in \mathbb{R}
\right\rbrace
\quad
dim(W)=2
\]

\[W=\left\lbrace
\begin{bmatrix}
a & b \\ c & 0
\end{bmatrix}
\mid
a, b, c\in \mathbb{R}
\right\rbrace
\quad
dim(U+W)=3
\]

\[U\cap W=\left\lbrace
\begin{bmatrix}
a & 0 \\ 0 & 0
\end{bmatrix}
\mid
a \in \mathbb{R}
\right\rbrace
\quad
dim(U\cap W)=1
\]

So we might have,
\[\begin{bmatrix}
1 & 1 & \\ 1 & 0
\end{bmatrix}
\in U+W
\text{ but, }
\begin{bmatrix}
1 & 1 & \\ 1 & 0
\end{bmatrix}
\notin U\cap W\]
\end{ex}

\begin{_prop}
Let $U$ and $W$ be subspaces of $V$.
Then,
\begin{enumerate}[i)]
 \item $U+W$ is a subspace; 
 \item $U$ and $W$ are contained in $U+W$;
 \item $U+W$ is the smallest subset containing $U$ and $W$, i.e. if $S$ is a subspace containing $U$ and $W$ then $U+W\subseteq S$ 
 \item $W+W=W$
\end{enumerate}
\end{_prop}

\begin{_thm}
Suppose that $U$ and $W$ are finite dimensional subspaces of $V$.
Then $U+W$ also has finite dimension and,
\[dim(U+W)=dim(U)+dim(W)-dim(U\cap W)\]
\end{_thm}

**Proof here**

\begin{_def}
The vector space $V$ is said to be the \textbf{direct sum} of its subspaces $U$ and $W$ denoted by $V=U\oplus W$, if every $v\in V$ can be written in one and only one way as $v=u+w$ with $u\in U, w\in W$.
\end{_def}

\begin{_thm}
The vector space $V$ is the direct sum of $U$ and $W$ iff (i) $V=U+w$ and (ii) $U\cap W=\{0\}$.
\end{_thm}

\begin{ex}
The follow are examples of vector space sums.
\begin{enumerate}[1)]
 \item 
  \[U=\{(a,b,0)\mid a,b\in \mathbb{R}\} \quad \text{[xy-plane]}\]
  \[W=\{(0,b,c)\mid b,c\in \mathbb{R}\} \quad \text{[yz-plane]}\]
  \[\mathbb{R}^3=U+W\]
  \begin{align*}
  U\cap W&=\{(0,b,0)\mid b\in \mathbb{R}\} \quad \text{[y-axis]} \\
  &\neq \{0\} \\
  (1,1,1)&=(1,1,0)+(0,0,1) \\
  &=(1,0,0)+(0,1,1)
  \end{align*}  
 \item
  \[U=\{(a,0,0)\mid a\in \mathbb{R}\} \quad \text{[x-axis]}\]
  \[W=\{(0,b,0)\mid b\in \mathbb{R}\} \quad \text{[y-axis]}\]
  \[U+W\quad \text{[xy-plane]}\]
  $U\cup W$ is just the union of the axis, it is not a subspace.
\end{enumerate}
\end{ex}

\chapter{Linear Transformations}

\section{Basic Definitions}

A transformation is of the form,
\[v\rightarrow Av\quad A\in M_{m\times n}, v\in \mathbb{R}^n, Av\in \mathbb{R}^m\]

It has the following operations,
\begin{itemize}
 \item $A(v+w)=Av+Aw$
 \item $k\in \mathbb{R}, A(kv)=k(Av)$
\end{itemize}

\section{Introduction to Linear Transformations}

\begin{_def}
Let $V$ and $U$ be vector spaces.
A function $T: V\rightarrow U$ is a \textbf{linear transformation} or linear mapping if the following two conditions are satisfied:
\begin{enumerate}[i)]
 \item For any vectors $v, w\in V,\, T(v+w)=T(v)+T(w)$.
  ($T$ preserves vector addition.)
 \item For any $v\in V$ and $k\in \mathbb{R}$, $T(kv)=kT(v)$
  ($T$ preserves scalar multiplication.)
\end{enumerate}
\end{_def}

\begin{_rem}
(i) and (ii) can be replaced by the following one.
For any $v,w\in V$ and $a,b\in \mathbb{R}$,
\[T(av+bw)=aT(v)+bT(w)\]
\end{_rem}

\begin{ex}
The following are examples for linear transformations.

\begin{enumerate}[1)]
 \item Multiplication by matrix $A\in M_{m\times n}$.
  \[L_A:\mathbb{R}^n\rightarrow\mathbb{R}^m \quad L_A(v)=Av \forall v\in \mathbb{R}^n\]
 \item Projection.
  \[P:\mathbb{R}^3\rightarrow \mathbb{R}^2\]
  This is the projection into the xy-plane.
  \begin{align*}
    P(x,y,z)&=(x,y,0) \\
    P(x,y,z)&=\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
    x \\ y \\ z
    \end{bmatrix}
  \end{align*}
  Let $v,w\in V$ and $a,b\in \mathbb{R}$,
  \begin{align*}
   v&=(v_1,v_2,v_3) \\
   w&=(w_1,w_2,w_3)
  \end{align*}
  \begin{align*}
  P(av+bw)&=P(av_1+bw_1,av_2+bw_2,av_3+bw_3) \\
  &=(av_1+bw_1,av_2+bw_2,0) \\
  &=(av_1,av_2,0)+(bw_1,bw_2,0) \\
  &=a(v_1,v_2,0)+b(w_1,w_2,0) \\
  &=aP(v_1, v_2, v_3)+bP(w_1,w_2,w_3) \\
  %=aP(v)+bP(w)
  \end{align*}
 \item Reflection.
  \[R:\mathbb{R}^2\rightarrow\mathbb{R}^2\]
  This is the reflection in the y-axis.
  \begin{align*}
   R(x,y)&=R(-x,y) \\
   &=\begin{bmatrix}
   -1 & 0 \\ 0 & 1
   \end{bmatrix}
   \begin{bmatrix}
   x \\ y
   \end{bmatrix}
  \end{align*}
 \item Derivatives.
  \[D:P(t)\rightarrow P(t)\]
  \[D(p(\cdot))=\frac{\mathrm{d}P}{\mathrm{d}t}(\cdot)\]
  For example, given $P(t)=1-t^2, D(P(t))=2t$.
 \item Integrals.
  \[J:P(t)\rightarrow \mathbb{R}\]
  \[J(D(\cdot))=\int_0^1 \! P(t) \, \mathrm{d}t\]
  Given $p(\cdot), q(\cdot)\in P(t), a, b\in \mathbb{R}$,
  \begin{align*}
   J(ap(\cdot)+bq(\cdot))&=\int_0^1 \! (ap(\cdot)+bq(\cdot))) \, \mathrm{d}t \\
   &=\int_0^1 \! ap(\cdot) \mathrm{d}t + \int_0^1 \! bq(\cdot) \, \mathrm{d}t \\
   &=a\int_0^1 \! p(\cdot) \mathrm{d}t + b\int_0^1 \! q(\cdot) \, \mathrm{d}t \\
   &=aJ(p(\cdot))+bJ(q(\cdot))
  \end{align*}
 \item Zero and Identity Transformations.
  Given vector spaces $V$ and $U$,
  \[\begin{array}{l l}
    \text{Zero transformation: }& \bar{0}: V\rightarrow U \\
    & \bar{0}(v)=0\in U \\
    \text{Identity transformation: }& 1_v: V\rightarrow V \\
    & 1_v(v)=v
    \end{array}\]
\end{enumerate}
\end{ex}

\begin{_rem}
If $T$ is a linear transformation then $T(0)=0$.
\end{_rem}
\begin{proof}
Take $k=0$ in (ii) of the definition of linear transformations.
\begin{align*}
T(kv)&=kT(v) \\
T(0v)&=0T(v) \\
T(0)&=0
\end{align*}
\end{proof}

\begin{_rem}
For any scalars $a_i\in \mathbb{R}$ and vectors $v_i\in V$,
\[T(a_1v_1+...+a_nv_n)=a_1T(v_1)+...+a_nT(v_n)\]
\end{_rem}

\begin{proof}
By induction.

Base case ($n=1$): $T(a_1v_1)=a_1T(v_1)$ [By (ii)]

Induction step ($n=k\Rightarrow n=k+1$):

Suppose the remark is true for $n=k$.
We want to show that the remark is true when $n=k+1$.

\begin{align*}
& T(a_1v_1+...+a_{k+1}v_{k+1}) \\
=& T((a_1v_1+...+a_kv_k)+a_{k+1}v_{k+1}) \\
=& T(a_1v_1+...+a_kv_k)+T(a_{k+1}v_{k+1}) \quad \text{[by (i)]}\\
=& a_1T(v_1)+...+a_kT(v_k)+T(a_{k+1}v_{k+1}) \quad \text{[Induction hypothesis, ]}n=k\\
=& a_1T(v_1)+...+a_kT(v_k)+a_{k+1}T(v_{k+1}) \quad \text{[by (ii)]}\\
\end{align*}
\end{proof}

\begin{_exc}
How many linear transformations $T:\mathbb{R}^2\rightarrow \mathbb{R}^2$ are there such that $T(1,0)=(-1,1)$ and $T(0,1)=(1,-1)$ exist?
\begin{align*}
T(x,y)=&T(x(1,0)+y(0,1)) \\
=&xT(1,0)+yT(0,1) \\
=&x(-1,1)+y(1,-1) \\
=&(-x,x)+(y,-y) \\
=&(-x+y, x-y)
\end{align*}

There is one and only one such linear transformation.
\end{_exc}

\begin{_prop}
Let $T:V\rightarrow U$ and $S:V\rightarrow U$ be two linear transformations.
Suppose that $\{v_1,...,v_n\}$ spans $V$.
If $T(v_i)=S(v_i) \, \forall i=1,...,n$, then $T=S$.
\end{_prop}

\begin{proof}
We want to show that,
\[T(v)=S(v)\, \forall v\in V\]
Let $v\in V$.
Since $\{v_1,...,v_n\}$ spans $V$, these exist scalars $a_1,...,a_n\in \mathbb{R}$ such that,
\[v=a_1v_1+...+a_nv_n\]
Then,
\begin{align*}
T(v)=&a_1T(v_1)+...+a_nT(v_n) \\
=&a_1S(v_1)+...+a_nS(v_n) \quad \text{[by condition of proposition]} \\
=&S(v)
\end{align*}
\end{proof}

\begin{ex}
The following is an example of what is not a linear transformation.

\[T:\mathbb{R}^2\rightarrow\mathbb{R}^2\]
Adds the vector $(1,2)$ to any vector $(x,y)\in\mathbb{R}^2$
\begin{align*}
T(x,y)=&(x,y)+(1,2)=(x+1,y+2) \\
\text{Case: }&\\
T(0,0)=&(1,2)\neq (0,0) \\
\text{Case: }&\\
T(2,2)=&(3,4)\\
2T(1,1)=&2(2,3)=(4,6)\neq (3,4)
\end{align*}
\end{ex}
\end{document}