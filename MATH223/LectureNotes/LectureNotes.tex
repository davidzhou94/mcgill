\documentclass{report} 
\usepackage{amsmath}
\usepackage{amsthm,amssymb,graphicx}
\usepackage{enumerate}
\usepackage[bookmarks]{hyperref}
\graphicspath{ {F:\repos\mcgill\MATH223\LectureNotes} }

\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

\newtheorem{_thm}{Theorem}
\theoremstyle{definition}
\newtheorem{_prob}{Problem}
\newtheorem{_sol}{Solution}
\newtheorem{_def}{Definition}
\newtheorem{_rem}{Remark}
\newtheorem{ex}{Example}
\newtheorem{_exc}{Exercise}
\newtheorem{_prop}{Proposition}

\begin{document} 
\title{MATH 223} 
\author{Yang David Zhou}
\date{Winter 2015}
\maketitle

\section{Administrativa}

\raggedright

Professor Tiago Salvador \newline

Website: http://www.math.mcgill.ca/tsalvador/ \newline

Office: Burnside Building 1036

Office Hours: M1:45-2:45PM W2:00-3:00PM, F3:30-4:30PM \newline

\textbf{Grading}

\begin{tabular}{ l l l }
  Assignments & 15\% & 15\% \\
  Midterm     & 25\% &  0\% \\
  Final       & 60\% & 85\% \\
\end{tabular} \newline

The midterm will be scheduled for the 7th week of class.

\chapter{Vectors}

\section[Vectors in Rn]{Vectors in \(\mathbb{R}^n\)}

\(\mathbb{R}^n\) is the set of all \(n\)-tuples of real numbers \(u=(a_1 ... a_n) \mid a\in \mathbb{R}\) where \(a\) are the \textbf{components} or \textbf{entries}.

\begin{_rem}
We use the term \textbf{scalar} to refer to an element in \(\mathbb{R}\).
\end{_rem}

\section{Basic Definitions}

\begin{_def}
\textbf{Addition}

\(u, v \in \mathbb{R}^n\)

\(u=(a_1...a_n)\) 

\(v=(b_1...b_n)\)

\(u+v=(a_1+b_1...a_n+b_n)\)
\end{_def}

\begin{_def}
\textbf{Scalar Multiplication}

\(k\in \mathbb{R}\)

\(ku=(ka_1...ka_n)\)
\end{_def}

\begin{_def}
Two vectors \(u\) and \(v\) are said to be \textbf{equal} (\(u=v\)) if \(a_i=b_i \forall i=1...n\).
\end{_def}

\begin{_def}
The \textbf{zero vector} is defined as \(0=(0...0)\).
\end{_def}

\begin{_def}
Suppose we are given \(m\) vectors \(u_1...u_m\in \mathbb{R}^n\) and \(m\) scalars \(k_1...k_m\in \mathbb{R}\).

Let \(u=k_1u_1+...+k_mu_m\).

Such a vector \(u\) is called a \textbf{linear combination} of the vectors \(u_1...u_m\).
\end{_def}

\begin{_def}
A vector \(u\) can be called a \textbf{multiple} of \(v\) if there is a scalar \(k\) such that \(u=kv\) with \(k\neq 0\). 
In the case \(k>0\) we say \(u\) is in the same direction as \(v\). 
In the case \(k<0\) we say \(u\) is in the opposite direction of \(v\).
\end{_def}

\section{The Dot Product}

\begin{_def}
Let \(u=(a_1...a_n)\) and \(v=(b_1...b_n)\). The \textbf{dot product} or inner product is given by,
\[u\cdot v=a_1b_1+...a_nb_n=\]
\end{_def}

\begin{_def}
The vectors \(u\) and \(v\) are \textbf{orthogonal} if \(u\cdot v=0\).
\end{_def}

\section{The Vector Norm}

\begin{_def}
The \textbf{norm} or \textbf{length} of a vector is given by,
\[\|u\|=\sqrt{a^2_1+...+a^2_n}\]
\end{_def}

Thus \(\|u\|\geq 0\) and \(\|u\|=0\) if and only if (iff) \(u=0\).

\begin{_def}
A vector is called a \textbf{unit vector} if \(\|u\|=1\).
\end{_def}

\begin{_def}
For any non-zero vector \(v\), the vector 
\[\hat{v}=\frac{1}{\|v\|} v\]
is the only unit vector with the same direction of $v$.
The process of finding \(\hat{v}\) is called \textbf{normalizing}.
\end{_def}

\section{Theorem: Cauchy-Schwarz Inequality}

\begin{_thm}
Given any two vectors \(u,v\in \mathbb{R}^n\), then,
\[|u\cdot v|\leq \|u\|\|v\|\]
\end{_thm}

\begin{proof}
Let \(t\in \mathbb{R}\). So, \(\|tu+v\|^2\geq 0\).
\begin{align*}
\|tu+v\|^2 &= (tu+v)(tu+v) \\
&= (tu\cdot tu)+(tu\cdot v)+(v\cdot tu)+(v\cdot v) \\
&= t^2(u\cdot u)+t(v\cdot u)+t(u\cdot v)+(v\cdot v) \\
&= t^2\|u\|^2+2t(u\cdot v)+\|v\|^2
\end{align*}
We can represent this in the form \(at^2+bt+c\geq 0\), so,
\[a=\|u\|^2, b=2(u\cdot v), c=\|v\|^2\]
Take the Discriminant as \(b^2-4ac\iff b^2\leq 4ac\).
\begin{align*}
4(u\cdot v)^2 &\leq 4\|u\|^2\|v\|^2 \\
|u\cdot v| &\leq \|u\|\|v\|
\end{align*}
\end{proof}

\section{Theorem: Minkowski Triangle Inequality}

\begin{_thm}
Given \(u,v\in \mathbb{R}^n\), then \(\|u+v\|\leq \|u\|+\|v\|\).
\end{_thm}

\begin{proof}
\begin{align*}
\|u+v\|^2 &= \|u\|^2+2(u\cdot v)+\|v\|^2 \\
&\leq \|u\|^2+2\|u\|\|v\|+\|v\|^2 \quad \text{ by C-S inequality} \\
&= (\|u\|+\|v\|)^2
\end{align*}
So, \(\|u+v\|^2\leq (\|u\|+\|v\|)^2\).
Take the square root and we are done.
\end{proof}

\section{Geometry with Vectors}

\begin{_def}
The \textbf{distance} between vectors \(u,v\in \mathbb{R}^n\) is given by,
\[d(u,v)=\|u-v\|=\sqrt{(a_1-b_1)^2+...+(a_n-b_n)^2}\]
\end{_def}

\begin{_def}
The \textbf{angle} between vectors \(u,v\in \mathbb{R}^n\) is given by,
\[cos\theta =\frac{u\cdot v}{\|u\|\|v\|} \quad \theta \in [0,\pi]\]
\end{_def}

Observe that in the previous definition, the angle is well defined.

\[-\|u\|\|v\|\leq -|u\cdot v|\leq u\cdot v\leq u\cdot v\leq |u\cdot v|\leq \|u\|\|v\|\]

Dividing the entire inequality by $\|u\|\|v\|$ yields,

\[-1\leq \frac{u\cdot v}{\|u\|\|v\|} \leq 1\]

\begin{_def}
A \textbf{hyperplane} $\mathcal{H}$ in $\mathbb{R}^n$ is the set of points $(x_1...x_n)$ that satisfy $a_1x_1+...+a_nx_n=b$ where $u=[a_1...a_n]\in \mathbb{R}^n$ and $b\in \mathbb{R}$.
\end{_def}

\begin{_def}
The \textbf{line} in $\mathbb{R}^n$ passing through a point $P=(b_1...b_n)$ and in the direction of $v\in \mathbb{R}^n$ with $v\neq 0$.
\[x=P+tu \quad t\in \mathbb{R}, \quad u=[a_1...a_n]\]
\[ \left\{ 
  \begin{array}{l}
    x_1=a_1t+b_1 \\
    x_n=a_nt+b_n
  \end{array} \right.\]
\end{_def}

\chapter{Algebra of Matrices}

\section{Introduction}

A matrix with $n$ rows and $m$ columns is written as,

\[A_{n\times m}=
\begin{bmatrix}
    a_{11} & \dots  & a_{1m} \\
    \vdots & \ddots & \vdots \\
    a_{n1} & \dots  & a_{nm}
\end{bmatrix}
\]

Or,

\[A_{n\times m}=[a_{ij}]\]

Where $a_{ij}$ is the entry in row $i$ and column $j$.

\section{Definitions and Properties of Matrices}

\begin{_def}
\textbf{Matrix Addition}
\[A+B=[a_{ij}+b_{ij}] \quad \forall i=1...n, j=1...m\]
\end{_def}

\begin{_def}
\textbf{Scalar Multiplication}
\[ka=[ka_{ij}] \quad \forall i=1...n, j=1...m\]
\end{_def}

\begin{_def}
\textbf{Zero Matrix}
\[0=[0]\]
\end{_def}

\begin{_def}
Given a matrix $A_{m\times p}$ and a matrix $B_{p\times n}$, \textbf{matrix multiplication} is defined as,
\[AB=[c_{ij}] \quad c_{ij}=\sum\limits_{k=1}^p a_{ik}b_{kj}\]
\end{_def}

\begin{_def}
Given a matrix $A$, its \textbf{transpose} is $A^T=[a_{ji}]$ where $A=[a_{ij}]$.
\end{_def}

\begin{_def}
A \textbf{square matrix} has the same number of rows as it does columns, i.e. $A_{n\times n}$ is a square matrix.
\end{_def}

\begin{_def}
Given a matrix $A=[a_{ij}]$ the elements in the \textbf{diagonal} are $[a_{11},...,a_{nn}]$.
\end{_def}

\begin{_def}
The \textbf{trace} of a matrix $A$ is given by,
\[tr(A)=a_{11}+...+a_{nn}\]
\end{_def}

\begin{_def}
The \textbf{identity matrix} $I_n$ is the matrix such that for any $n$-square matrix $A$,
\[AI=IA=A\]
\end{_def}

\begin{_def}
The \textbf{Kronecker delta} is defined by,
\[\delta_{ij}=\left\{ 
  \begin{array}{l l}
    0 & \quad \text{if } i\neq j\\
    1 & \quad \text{if } i=j
  \end{array} \right.\]
\end{_def}

\begin{_rem}
Given the definitions for the identity matrix and the Kronecker delta, an alternative definition for the identity matrix is as follows,
\[I=[\delta_{ij}]\]
\end{_rem}

\begin{_def}
A matrix $A$ is \textbf{invertible} if there is a matrix $B$ such that $AB=BA=I$.
\end{_def}

\begin{_rem}
In general, for any matrices $A$ and $B$, $AB\neq BA$.
\end{_rem}

\begin{_def}
A matrix $D$ is \textbf{diagonal} if all the non-zero entries are in the diagonal.
\[D=diaginal(d_1,...,d_n)\]
\end{_def}

\begin{_def}
A matrix $A$ is \textbf{upper triangular} if,
\[a_{ij}=0 \quad \forall i>j\]
\end{_def}

\section{Complex Numbers}

The imaginary number $i$ is defined as $i=\sqrt{-1}$ or equivalently, $i^2=-1$.

\begin{_def}
A \textbf{complex number} $z$ is given by,
\[z=a+bi \quad a,b\in \mathbb{R}\]
Where $a$ is the real part and $b$ is the imaginary part.
\end{_def}

Real numbers are also complex numbers with no imaginary component, i.e. $a+0i=a$. \newline

\textbf{Addition} for two complex numbers $z=a+bi$ and $w=c+di$ is given by,
\[z+w=(a+c)+(b+d)i\]
\textbf{Multiplication} for the same two complex numbers is given by,
\begin{align*}
z\cdot w &= (a+bi)(c+di) \\
&= ac+adi+cbi-bd \\
&= (ac-bd)+(ad+bc)i
\end{align*}

\begin{_def}
The \textbf{conjugate} of $z=a+bi$ is $\bar{z}=a-bi$.
\end{_def}

\begin{_def}
The \textbf{absolute value} or modulus of $z=a+bi$ is $|z|=\sqrt{a^2+b^2}$.
\end{_def}

\begin{ex}
\[z^{-1}=\frac{1}{z}=\frac{1}{a+bi}\cdot\frac{a-bi}{a-bi}=\frac{a-bi}{a^2+b^2}\]
\end{ex}

Observe that the following properties are true for conjugates and absolute values,
\begin{enumerate}
 \item \[z\bar{z}=|z|^2=a^2+b^2\]
 \item \[\bar{z\pm w}=\bar{z}\pm \bar{w}\]
 \item \[\bar{zw}=\bar{z}\cdot \bar{w}\]
 \item \[\bar{(\bar{z})}=z\]
 \item $z$ is real iff $z=\bar{z}$
 \item \[|zw|=|z||w|\]
 \item \[|z+w|\leq |z|+|w|\]
\end{enumerate}

\chapter{Systems of Linear Equations}

\section{Representing Linear Systems with Matrices}

Given a system of linear equations of the form,

\[\left\{ 
  \begin{array}{c}
    a_{11}x_1+...+a_{1m}x_n=b_1 \\
    \vdots \\
    a_{m1}x_1+...+a_{mn}x_n=b_m
  \end{array} \right.\]

\begin{itemize}
 \item $x_1,...,x_m$ are the unknowns, and
 \item $a_{ij}$ and $b_i$ are the constants.
\end{itemize}

The system can also be represented by matrices where,

\begin{itemize}
 \item $A=[a_{ij}]$ is the matrix of coefficients
 \item $b=[b_i]$ is the column vector of constant
 \item $M=[A|b]$ is the matrix that represents the system.
\end{itemize}

\begin{_def}
A matrix $A$ is in \textbf{echelon form} if 
\begin{enumerate}
 \item all zero rows are at the bottom, and
 \item each leading non-zero entry in a row is to the right of the leading non-zero entry in teh preceding row.
\end{enumerate}
\end{_def}

\begin{ex}
This matrix is in echelon form.
The boxed entries are the \textbf{pivots}.

\[A=
\begin{bmatrix}
 0 & \boxed{2} & 3 & 4 & 1 & 0 & 6 \\
 0 & 0 & 0 & \boxed{2} & 1 & 2 & 1 \\
 0 & 0 & 0 & 0 & 0 & \boxed{1} & 1 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\]
\end{ex}

\begin{_def}
A matrix is said to be in teh \textbf{row-reduced echelon form} if it is in the echelon form and,
\begin{enumerate}
 \item each pivot is equal to 1, and
 \item each pivot is the only non-zero entry in its column
\end{enumerate}
\end{_def}

\begin{ex}
This matrix is in row-reduced echelon form.
The boxed entries are the \textbf{pivots}.

\[A=
\begin{bmatrix}
 0 & \boxed{1} & 3 & 0 & 0 & 4 \\
 0 & 0 & 0 & \boxed{1} & 0 & -3 \\
 0 & 0 & 0 & 0 & \boxed{1} & 2 \\
\end{bmatrix}
\]
\end{ex}

\section{Elementary Row Operations}

Suppose that $A$ is a matrix with rows $R_1,...,R_m$.
The elementary row operations that can be performed on $A$ are as follows,

\begin{enumerate}
 \item Row interchange, $R_i\leftrightarrow R_j$
 \item Row scaling, $kR_i\rightarrow R_i$
 \item Row addition, $kR_i+R_j\rightarrow R_j$
\end{enumerate}

The method by which we find the (row-reduced) echelon form of a matrix is using the \textbf{Gaussian Elimination} algorithm. \newline

Recall that every matrix is row equivalent to a unique matrix in the row-reduced echelon form.

\begin{_def}
The \textbf{rank} of a matrix $rank(A)$ is the number of pivots in the row-reduced echelon form.
There are many other ways to define rank but they all have the same meaning.
\end{_def}

The method by which we find the inverse of a square matrix $A$ is as follows,

Let $M=[A\mid I]$. 
Find the row-reduced echelon form of $M$.
If there is a zero row in the resulting matrix then $A$ is not invertible.
Otherwise, $M\sim [I\mid B], \quad A^{-1}=B$.

\begin{_thm}
Let $A$ be a square matrix.
The following conditions are equivalent,
\begin{enumerate}
 \item $A$ is invertible
 \item the row-reduced echelon for of $A$ is $I$
 \item the only solution to $Ax=0$ is $x=0$
 \item the system $Ax=b$ has a solution for any choice of column $b$.
\end{enumerate}
\end{_thm}

A partial proof is as follows,
\begin{proof}
$(1)\Rightarrow (3)$ There is a matrix $B$ such that $AB=I=BA$.

Let $x$ be any solution of $Ax=[0]$.
\begin{align*}
BAx &= B[0] \\
Ix &= [0] \\
x &= [0]
\end{align*}

$(1)\Rightarrow (4)$ Fix a column $b$,
\begin{align*}
Ax &= b \\
\Leftrightarrow \quad A^{-1}Ax &= A^{-1}b \\
\Leftrightarrow \quad x &= A^{-1}b
\end{align*}
\end{proof}

\begin{_def}
A linear system $Ax=b$ is \textbf{homogeneous} if $b=0$.
Otherwise, $Ax=b$ is said to be \textbf{non-homogeneous}.
\end{_def}

\begin{_def}
A \textbf{particular solution} of $Ax=b$ is a vector $x$ such that $Ax=b$.
The set of all particular solutions is called the \textbf{general solution} of the solution set.
\end{_def}

\begin{_def}
A system $Ax=b$ is \textbf{consistent} if it has one or more solutions and it is said to be \textbf{inconsistent} if it has no solutions.
\end{_def}

\begin{_thm}
Any system $Ax=b$ has:

(i) an unique solution, 

(ii) no solution, or

(iii) an infinite number of solutions.
\end{_thm}

\section{Examples}

\begin{ex}
The system,
\begin{align*}
x+y+2z&=1 \\
3x-y+z&=-1 \\
-x+3y+4z&=1
\end{align*}
is equivalent to,
\[
\begin{amatrix}{3}
   1 & 1 & 2 & 1 \\
   3 & -1 & 1 & -1 \\
   -1 & 3 & 4 & 1
\end{amatrix}\sim
\begin{amatrix}{3}
   1 & 1 & 2 & 1 \\
   0 & 4 & 5 & 4 \\
   0 & 0 & 1 & -2
\end{amatrix}\]
\end{ex}

\begin{ex}
Back substitution:

$z=-2$

$4y+5z=4\Leftrightarrow y=\frac{7}{2}$

$x+y+2z=1\Leftrightarrow x=\frac{3}{2}$

\[\sim
\begin{amatrix}{3}
 1 & 0 & 0 & \frac{3}{2} \\
 0 & 1 & 0 & \frac{7}{2} \\
 0 & 0 & 1 & -2
\end{amatrix}
\]
\end{ex}

\begin{ex}
\begin{align*}
-2x+3y+3z&=-9 \\
3x-4y+z&=5 \\
-5x+7y+2z&=-14
\end{align*}

\[\sim
\begin{amatrix}{3}
 -2 & 3 & 3 & -9 \\
 3 & -4 & 1 & 5 \\
 -5 & 7 & 2 & -14
\end{amatrix}\sim
\begin{amatrix}{3}
 1 & -1 & 4 & -4 \\
 0 & 1 & 11 & -17 \\
 0 & 0 & 0 & 0
\end{amatrix}
\]

So there are infinitely many solutions.

Set $z=t$ since $z$ is a free variable, then back substitute.

\[y=-17-11t\quad x=-21-15t\quad t\in \mathbb{R}\]

So the solution space is,
\[\begin{bmatrix}
 x \\
 y \\
 z
\end{bmatrix} =
\begin{bmatrix}
 -21 \\
 -17 \\
 0
\end{bmatrix} +t
\begin{bmatrix}
 -15 \\
 -11 \\
 1
\end{bmatrix}
\]

Where $(-21, -17, 0)$ is a particular solution and $(-15, -11, 1)$ is the set of basic solutions of the homogeneous system $Ax=0$.
\end{ex}

\begin{ex}
\begin{align*}
x+2y-z&=2 \\
2x+5y-3z&=1 \\
x+4y-3z&=3
\end{align*}
\[\begin{amatrix}{3}
 1 & 2 & -7 & 2 \\
 2 & 5 & -3 & 1 \\
 1 & 4 & -3 & 3
\end{amatrix}\sim
\begin{amatrix}{3}
 1 & 2 & -1 & 2 \\
 0 & 1 & -1 & -3 \\
 0 & 0 & 0 & -1
\end{amatrix}
\]

There are no solutions possible for this system.
\end{ex}

\chapter{Vector Spaces}

\section{Introduction}

Adding two vectors in $\mathbb{R}^n$ produces a vector in $\mathbb{R}^n$.
Similarly, multiplying by a scalar produces a vector in $\mathbb{R}^n$.
These are some properties of a vector space, the following section is a formal list.

\section{Basic Definitions}

\begin{_def}

Let $V$ be a non-empty set with two operations,
\begin{enumerate}[i]
 \item Vector addition: this assigns to any $u,v\in V$ the sum $u+v\in V$.
 \item Scalar multiplication: this assigns to any $u\in V$ and $k\in K$, a product $ku\in V$ where $k$ is a field.
\end{enumerate}
Then $V$ is called a \textbf{vector} space (over the field $K$) if the following axioms hold for any $u,v,w\in V$.
\begin{itemize}
 \item A1) $(u+v)+w=u+(v+w)$
 \item A2) there is a vector in $V$ denoted by $0$ called the zero vector such that $v+0=v$ for any $v\in V$.
 \item A3) for each $u\in V$, there is a vector in $V$ denoted $-u$, such that $u+(-u)=0$.
 $-u$ is called the negative of $u$.
 \item A4) $u+v=v+u$
 \item A5) $k(u+v)=ku+kv$ for any scalar $k\in K$
 \item A6) $(a+b)u=au+bu$ for any scalars $a,b\in K$
 \item A7) $(ab)u=a(bu)$ for any scalars $a,b\in K$
 \item A8) $1u=u$ for the unit scalar $k\in K$
\end{itemize}
\end{_def}

\begin{_rem}
A \textbf{field} $K$ is a mathematical object with nice properties, with $\mathbb{R}$ and $\mathbb{C}$ being two examples. From now on, we will take it to be $\mathbb{R}$ or $\mathbb{C}$.
\end{_rem}

\section{Examples of Vector Spaces}

\begin{ex}
These are some examples of vector spaces,
\begin{enumerate}
 \item $\mathbb{R}^n$
 \item $\mathbb{C}^n$
 \item The matrix space: $M_{m\times n}$
 
 $M_{m\times n}$ denotes the set of all matrices with size $m$ rows, $n$ columns and real entries.
 $M_{m\times n}(\mathbb{C})$ permits the entries to be complex.
 The space of the real matrices are a subset of the space of complex matrices.
 \item The polynomial space: $P(t)$
 
 $P(t)$ denotes the set of all polynomials of the form,
 \[P(t)=a_0+a_1t+...+a_nt^n \mid a_i\in \mathbb{R}\]
 \item The function space: $F(x)$
 
 Let $X$ be a non-empty set.
 Let $F(x)$ denote the set of all functions of $X$ into $\mathbb{R}$.
 Then $F(x)$ is a vector space (over $\mathbb{R}$) with respect to the following operations,
 \begin{enumerate}[i]
  \item vector addition: 
  \[(f+g)(x)=f(x)+g(x)\mid \forall x\in X\]
  \item scalar multiplication: for any $k\in K, f\in F(x)$
  \[(kf)(x)=kf(x) \mid \forall x\in X\]
  \item zero function: $\underline{0}(x)=0$ 
 \end{enumerate}
\end{enumerate}
\end{ex}

\begin{_exc}
Consider the set $\mathbb{R}^2$ with the usual scalar multiplication, but with the following vector addition:
\[(a,b)\diamond (c,d)=(a+d,b+c)\]

Is this a vector space?

No because axiom 4 does not hold.

\[(1,2)\diamond (-1,1)=(2,1)\]
\[(-1,1)\diamond (1,2)=(1,2)\]
\end{_exc}

\section{Vector Subspaces}

\begin{_def}
Let $V$ be a vector space and $W$ be a subset of $V$.
Then $W$ is a \textbf{subspace} of $V$ if $W$ itself is a vector space with the operations of vector addition and scalar multiplication of $V$.
\end{_def}

\begin{ex}
$P(t)$ is a subspace of $F(\mathbb{R})$
\end{ex}

The next theorem provides a simple criteria to show that a subset $W$ of $V$ is a subspace.

\begin{_thm}
Suppose that $W$ is a subset of $V$, with $V$ being a vector space. Then $W$ is a subspace if the following two conditions hold:
\begin{enumerate}[i]
 \item The zero vector $0$ belongs to $W$.
 \item For every two vectors $u,v\in W$ and $k\in R$
 \begin{itemize}
  \item $u+v\in W$ (closed under vector addition)
  \item $ku\in W$ (closed under scalar multiplication)
 \end{itemize}
\end{enumerate}
\end{_thm}

\begin{proof}
By (i), $W$ is non-empty.

By (ii), the operations of vector addition and scalar multiplication are well defined.

The it remains to prove each of the axioms of a vector space.

A1, 4, 5, 6, 7, and 8 hold in $W$ because they hold in $V$.

A2 is true by (i).

A3: Let $v\in W$.
We know that $-v\in V$ with $v+(-v)=0$ by A3 for the vector space $V$.
But $W$ is closed under scalar multiplication (by (ii)) and so $v\in W$ and we are done.
\end{proof}

\section{Examples of Vector Subspaces}

\begin{ex}
These are some examples of vector subspaces,
\begin{enumerate}
 \item ${0}, V$ are subspaces of $V$.
 These are called the trivial subspaces of $V$.
 \item Subspaces of $\mathbb{R}^3$
 \begin{enumerate}[i]
  \item Line through the origin is a subspace.
  \item Planes through the origin.
 \end{enumerate}
 \item Subspaces of $P(t)$
 \begin{enumerate}[i]
  \item $P_m(t)=\{p(\cdot)\in P(t); degree(p(\cdot))\leq m\}$
  \item $Q(t)$ is the set of polynomials with only even powers
 \end{enumerate}
 \item Subspaces of matrices $M_{m\times n}$
 \begin{enumerate}[i]
  \item $W_1=\{A\in M_{m\times n}; A \text{ is diagonal}\}$
  \item $W_2=\{A\in M+{m\times n}; A=A^T\}$
 \end{enumerate}
 \item Subspaces of $F(\mathbb{R})$
 \begin{enumerate}[i]
  \item $C(\mathbb{R})=\{f\in F(\mathbb{R}); f \text{ is continuous}\}$
  \item $C'(\mathbb{R})=\{f\in F(\mathbb{R}); f \text{ is differentiable}\}$
 \end{enumerate}
\end{enumerate}
\end{ex}

\section{More on Vector Spaces}

\begin{_def}
Let $A\in M_{m\times n}$. The nullspace of $A$ is $N(A)$ which is given by,
\[N(A)=\{x\in \mathbb{R}^n \mid Ax=0\}\]
\end{_def}

\begin{_prop}
$N(A)$ is a subspace of $\mathbb{R}^n$
\end{_prop}
\begin{proof}
Clearly $N(A)$ is a subset of $\mathbb{R}^n$
\begin{enumerate}[i]
 \item $0\in N(A)$. True because $A0=0$.
 \item Let $u,v\in N(A)$, and $a,b\in \mathbb{R}$.
 We want to show that $au+bv\in N(A)$, which is the same as $A(au+bv)=0$
 \begin{align*}
 A(au+bv) &= A(au)+a(bv) \\
 &= a(Au)+b(Av) \quad \text{Since } u,v\in N(A) \text{, then } Au=0, Av=0 \\
 &= a0+b0 \\
 &= 0
 \end{align*}
\end{enumerate}
\end{proof}

\begin{_rem}
The solution set of a non-homogeneous system $\{x\in \mathbb{R}^n \mid Ax=b\}$ where $b\neq 0$ is not a subspace because the zero vector is not present.
\end{_rem}

\begin{_thm}
Let $U$ and $W$ be subspaces of a vector space $V$.
Then $U\cap W$ is also a subspace.
\end{_thm}

\begin{proof}
Since $U\subseteq V$ and $W\subseteq V$ ($U$ and $W$ are subspaces),
\[U\cap W\subseteq V\]
\begin{enumerate}[i]
 \item So, $0\in V$ and $0\in W$, therefore $0\in U\cap W$
 \item Let $u,v\in U\cap W$ and $a,b\in \mathbb{R}$
 \begin{align*}
  u,v\in U\cap W &\Rightarrow \left\{ 
  \begin{array}{l}
    u,v\in V \\
    u,v\in W
  \end{array} \right. \\
  &\Rightarrow \left\{ 
  \begin{array}{l}
    au+bv\in V \\
    au+bv\in W
  \end{array} \right. \quad \text{both } U \text{ and } W \text{are subspaces} \\
  &\Rightarrow au+bv\in U \cap W
 \end{align*}
\end{enumerate}
\end{proof}

\begin{_rem}
In general, if $U$ and $W$ are subspaces, $U\cup W$ is \textbf{not} a subspace.
An example would be two lines through the origin in $\mathbb{R}^3$.
\end{_rem}

\section{Linear Combinations}

Observe that $au+bv$ is a linear combination.

\begin{_def}
Let $U$ be a vector space.
A vector $v\in V$ is a \textbf{linear combination} of $u_1,...,u_m$ in $V$ if there exists scalars $a_1,...,a_m$ so that,
\[v=a_1u_1+...+a_mu_m\]
\end{_def}

\begin{ex}
The following is an example of linear combinations in $\mathbb{R}^3$.

Is $v=(1,5,5)\in \mathbb{R}^3$ a linear combination of $u_1=(1,2,3)$, $u_2=(1,0,1)$, $u_3=(0,1,0)$?

That is the same as asking, are there constants $a, b, c\in \mathbb{R}$ such that,
\[v=au_1+bu_2+cu_3\]

That is, are there $a,b,c\in \mathbb{R}$ such that,

\[\begin{bmatrix}
1 \\ 5 \\ 5
\end{bmatrix}=a\begin{bmatrix}
1 \\ 2 \\ 3
\end{bmatrix}+b\begin{bmatrix}
1 \\ 0 \\ 1
\end{bmatrix}+c\begin{bmatrix}
0 \\ 1 \\ 0
\end{bmatrix}\]

Yes. Take $a=2, b=-1, c=1$.
\end{ex}

\begin{_def}
Let $A\in M_{m\times n}$.
The column space of $A$ is $C(A)$ which consists of all linear combinations of the columns of $A$.
Alternatively,
\[C(A)=\{Ax\mid x\in \mathbb{R}^n\}\]
\end{_def}

\begin{_prop}
The linear system $Ax=b$ is consistent iff $b\in C(A)$.
\end{_prop}

\begin{ex}
The following is an example of linear combinations in $P(t)$.

Is the polynomial $P(t)=t^2+5t+5$ a linear combination of the polynomials $P_1(t)=t^2+2t+3, P_2(t)=t^2+1, P_3(t)=t$?

Equivalently, are there scalars $a,b,c\in \mathbb{R}$ such that,
\[p(\cdot)=ap_1(\cdot)+bp_2(\cdot)+cp_3(\cdot)\]

There are two ways of solving this,
\begin{enumerate}
 \item Matching coefficients:
 \[t^2+5t+5=(a+b)t^2+(2a+c)t+3a+b\]
 \[\left\{ 
  \begin{array}{l}
    1 = a+b \\
    5=2a+c \\
    5=3a+b
  \end{array} \right. \Leftrightarrow
  \begin{bmatrix}
  1 & 1 & 0 \\
  2 & 0 & 1 \\
  3 & 1 & 0
  \end{bmatrix}
  \begin{bmatrix}
  a \\ b \\ c
  \end{bmatrix}=
  \begin{bmatrix}
  1 \\ 5 \\ 5
  \end{bmatrix}\]
 \item "Trial approach":
 We set $t$ in $P(t)$ equal to the three distinct values and each one provides a different equation,
 \[\begin{array}{l l}
 t=0 & 5=3a+b \\
 t=1 & 11=6a+2b+c \\
 t=-1 & 1=2a+2b-c
 \end{array}\]
 Then solve for $a, b, c$.
\end{enumerate}
\end{ex}

\begin{ex} The following are two examples of subspaces of $\mathbb{R}^3$
\begin{enumerate}
 \item A line with direction $(1,2,3)$ through the origin,
 \[\{t\begin{bmatrix}
 1 \\ 2 \\ 3
 \end{bmatrix} \mid t\in \mathbb{R} \}\]
 \item A plane through the origin,
 \begin{align*}
 \{(x,y,z)\in \mathbb{R}^3\mid z=0\}
 &=\{t(1,0,0)+s(0,1,0)\mid t,s\in \mathbb{R}\} \\
 &=\{(t,s,0)\mid t,s\in \mathbb{R}\}
 \end{align*}

\end{enumerate}
\end{ex}

\section{The Span of a Vector Space}

\begin{_def}
Let $u_1,...,u_m$ be vectors in $V$.
The set of all linear combinations of $u_1,...,u_m$ is called the \textbf{span} of $u_1,...,u_m$ and is denoted by $span\{u_1,...,u_m\}$.
\[span\{u_1,...,u_m\}=\{t_1u_1+...+t_mu_m\mid t_1,...,t_m\in \mathbb{R}\}\]
\end{_def}

\begin{_def}
The vectors $u_1,...,u_m\in V$ are said to span $V$ or to form a \textbf{spanning set} of $V$ if,
\[span\{u_1,...,u_m\}=V\]
\end{_def}

The following are the properties of spans.
\begin{itemize}
 \item If $span\{u_1,...,u_m\}=V$, then for any $v\in V$, $span\{v,u_1,...,u_m\}=V$.
 \item If $span\{0,u_1,...,u_m\}=V$, then $span\{u_1,...,u_m\}=V$.
 \item If $span\{u_1,...,u_m\}=V$ and $u_k$ is a linear combination of $u_1,...,u_{k-1},u_{k+1},...,u_m$ then $span\{u_1,...,u_{k-1},u_{k+1},...,u_m\}=V$.
\end{itemize}

\begin{_prop}
Let $u_1,...,u_m$ be vectors in $V$.
Then $span\{u_1,...,u_m\}$ is a subspace.
\end{_prop}

\begin{proof}
Clearly $span\{u_1,...,u_m\}\subseteq V$.

We know $0\in span\{u_1,...,u_m\}$ since,
\[0=0u_1+...+0u_m\in span\{u_1,...,u_m\}\]
Take any $u,v\in span\{u_1,...,u_m\}$ and $a,b\in \mathbb{R}$.
\[u=a_1u_1+...+a_mu_m \quad a_1,...,a_m\in\mathbb{R}\text{ since } u\in span\{u_1,...,u_m\}\]
Likewise,
\[v=b_1u_1+...+b_mu_m \quad b_1,...,b_m\in\mathbb{R}\]
So,
\begin{align*}
au+bv&=aa_1u_1+...+aa_mu_m+bb_1u_1+...+bb_mu_m \\
&=(aa_1+bb_1)u_1+...+(aa_m+bb_m)u_m
\end{align*}
Which shows that $au+bv$ is a linear combination of $u_1,....u_m$ with scalars $aa_1+bb_1,...,aa_m+bb_m$.
\end{proof}

\begin{_exc}
$W=\{(x,y,z)\in \mathbb{R}^3\mid x=2y=3z\}$

Clearly, $W\subseteq \mathbb{R}^3$

$(0,0,0)\in W$ is true because $0=2(0)=3(0)$.

$u,v\in W\quad a,b\in \mathbb{R}$

$u=(u_1,u_2,u_3)$ with (1) $u_1=2u_2$ and (2) $2u_2=3u_3$

$v=(v_1,v_2,v_3)$ with (3) $v_1=2v_2$ and (4) $2v_2=3v_3$

\begin{align*}
z=(z_1,z_2,z_3)&=au+bv \\
&=(au_1+bv_1,au_2+bv_2,au_3+bv_3)
\end{align*}

We want to show $z\in W$, so,
\begin{align*}
z_1&=2z_2=3z_3 \\
z_1&=au_1+bv_1 \\
&=a(2u_2)+b(2v_2) \quad \text{by (1) and (3)}\\
&=2(au_2+bv_2) \\
&=2z_2 \\
&=a(3u_3)+b(3v_3) \\
&=3(au_3+bv_3) \\
&=3z_3
\end{align*}
So, $z_1=2z_2=3z_3$.

It is also a line through the origin,
\[x=2y=3z\Rightarrow\]
\[\{t\begin{bmatrix}
6 \\ 3 \\ 2
\end{bmatrix}\mid t\in \mathbb{R}\}=span\{(6,3,2)\}\]
\end{_exc}

\begin{_def}
The \textbf{span of a set} $S$ is the set of all linear combinations of vectors in $S$.
If $S\neq 0$, $span(S)=\{0\}$.
\end{_def}

\begin{_thm}
Let $S$ be a subset of the vector space $V$.
Then,
\begin{enumerate}[i)]
 \item $span(S)$ is a subspace of $V$.
 \item if $W$ is a subspace of $V$ such that $S\subseteq W$, then $span(S)\subseteq W$.
\end{enumerate}
\end{_thm}

*Proof here*

\begin{_def}
Let $A\in M_{m\times n}$.
The \textbf{row space} of $A$, written as $rowsp(A)$, is the set of all linear combinations of rows of $A$.
\[rowsp(A)=col(A^T)\]
The notation for column space can also be $colsp(A^T)$.
$A\in M_{m\times n}$.
$rowsp(A)$ is a subspace of $\mathbb{R}^n$.
$col(A)$ is a subspace of $\mathbb{R}^m$.
\end{_def}

\begin{ex}
Two matrices are row equivalent if you can get from one to the other with only elementary row operations.
\[A=
\begin{bmatrix}
1 & 2 & -1 & 3 \\
2 & 4 & -1 & 5 \\
3 & 6 & -2 & 8
\end{bmatrix}\sim
\begin{bmatrix}
1 & 2 & -1 & 3 \\
0 & 0 & 1 & -1 \\
0 & 0 & 0 & 0
\end{bmatrix}=B
\]
The following is true for $A$ and $B$,
\begin{itemize}
 \item $rowsp(A)=rowsp(B)$
 \item $rowsp(A)=span{(1,2,-1,3),(2,4,-1,5),(3,6,-2,8)}$
 \item $rowsp(B)=span{(1,2,-1,3),(0,0,1,-1)}$
\end{itemize}
Observe that any basis of a subspace is not unique.
\end{ex}

\begin{_thm}
Row equivalent matrices have the same row space.
\end{_thm}

\section{Linear Dependence and Independence}

\begin{_def}
The vectors $v_1,...,v_n$ are \textbf{linearly independent} if the following condition is satisfied,
\[\text{if }a_1v_1+...+a_nv_n=0\text{ , then }a_1=...=a_n=0\]
The vectors $v_1,...,v_n$ are \textbf{linearly dependent} if they are not linearly independent.
\end{_def}

\begin{_rem}
Consider the vector equation,
\[x_1v_1+...+x_nv_n=0\]
where $x_1,...,x_n$ are unknown scalars.
If the only solution is $(0,..,0)$, then the vectors are linearly independent.
Otherwise they are linearly dependent.
\end{_rem}

\begin{ex}
The following is an example of linear dependence in $\mathbb{R}^3$.

Geometrically, linearly dependent vectors run in the same direction.
\begin{enumerate}[i)]
 \item Two vectors in $\mathbb{R}^3$ are linearly dependent if they lie on the same line.
 i.e., $k\in \mathbb{R}, k\neq 0$
 \[v_2=kv_1 \Leftrightarrow kv_1-v_2=0\]
 \item Three vectors in $\mathbb{R}^3$ are linearly dependent if they lie on the same plane.
 i.e., $a_1,a_2\in \mathbb{R}, a_1,a_2\neq 0$
 \[v_3=a_1v_1+a_2v_2 \Leftrightarrow a_1v_1+a_2v_2-v_3=0\]
\end{enumerate}
\end{ex}

\begin{_def}
An \textbf{infinite set of vectors} $S$ is linearly dependent if there exist vectors $v_1,...,v_n\in S$ that are linearly dependent.
\end{_def}

\begin{_prop}
Let $V$ be a vector space.
\begin{enumerate}[i)]
 \item If $v\neq 0, \{v\}$ is linearly independent.
 \item No independent set of vectors contains the zero vector.
 Any non-zero scalar multiplied by the zero vector will still yield the zero vector.
 \item \underline{Two} vectors are linearly dependent iff one of them is a multiple of the others.
 Let $v_1, v_2$ be linearly dependent and $a_1\neq 0$.
 \begin{align*}
 &a_1v_1+a_2v_2=0 \\
 \Rightarrow & v_1+\frac{a_2}{a_1}v_2=0 \\
 \Leftrightarrow & v_1=\frac{-a_2}{a_1}v_2
 \end{align*}
 \item No independent set can contain two vectors that are multiples of each other.
\end{enumerate}
\end{_prop}

\begin{_exc}
Show that ${1+t,3t+t^2,2+t-t^2}$ is linearly independent in $P_2(t)$.

Suppose,
\[a(1+t)+b(3t+t^2)+c(2+t-t^2)=0\quad \forall t\in \mathbb{R}\]
We want to show $a=b=c=0$.

Substitute three different values for $t$ to obtain three equations, then solve.
\begin{align*}
t=0\quad & a+b(0)+2(c)=0\Leftrightarrow a+2c=0\\
t=-1\quad & -2b=0\Leftrightarrow b=0\\
t=1\quad & 2a+0+2c=0\Leftrightarrow 2a+2c=0\quad \text{zero term from }t=-1
\end{align*}
So, $a+2c=0$ (1) and $2a+2c=0$ (2).
(2)$-$(1) gives $a=0$.

And we are done.

The alternate method is to match coefficients and solve that system as in the example in 4.7.
\end{_exc}

\begin{_prop}
The vectors $v_1,...,v_n$ are linearly dependent iff one of them is a linear combination of another.
\end{_prop}

\begin{proof}
($\Rightarrow$)

The vectors are linearly dependent.
Then, $\forall a_1,...,a_n \exists a_i\neq 0$ such that $a_1v_1+...+a_nv_n=0$.

Say $a_k\neq 0$.
\begin{align*}
& a_1v_1+...+a_nv_n=0 \\
\Leftrightarrow &\frac{a_1}{a_k}v_1+...+\frac{a_{k-1}}{a_k}+v_{k-1}+v_k+\frac{a_{k+1}}{a_k}v_{k+1}+...+\frac{a_n}{a_k}v_n \\
\Leftrightarrow &v_k=-\frac{a_1}{a_k}v_1-...-\frac{a_{k-1}}{a_k}-v_{k-1}-\frac{a_{k+1}}{a_k}v_{k+1}-...-\frac{a_n}{a_k}v_n
\end{align*}
Observe that $v_k$ can be any vector including the zero vector.

($\Leftarrow$)

Say $v_i$ is a linear combination of the other vectors.
Then,
\begin{align*}
& v_i=b_1v_1+...+b_{i-1}v_{i-1}+b_{i+1}v_{i+1}+...+b_nv_n \\
\Leftrightarrow & 0=b_1v_1+...+b_{i-1}v_{i-1}-v_i+b_{i+1}v_{i+1}+...+b_nv_n
\end{align*}
So, the scalar on $v_i$ is $-1$ which is not zero.

And we are done.
\end{proof}

\section{Basis}

\begin{_def}
A set $B=\{u_1,...,u_n\}$ of vectors in $V$ is a \textbf{basis} of $V$ if two conditions are satisfied,
\begin{enumerate}[i)]
 \item $B$ is a linearly independent set
 \item $span(B)=V$
\end{enumerate}
\end{_def}

\begin{ex}
The following are examples of basis,
\begin{enumerate}[1)]
 \item $\mathbb{R}^n$
 \[e_1=(1,0,...,0), e_2=(0,1,0,...,0), e_n=(0,...,0,1)\]
 $\{e_1,...,e_n\}$ is a basis for $\mathbb{R}^n$.

 $(1,2,3)=e_1+2e_2+3e_3$. This is the canonical or standard basis.
 \item $P_m(t)$
 \[\{1,t,t^2,...,t^m\}\]
 \item $M_{m\times n}$
 For $M_{2\times 3}$ it is,
 \[
 \begin{bmatrix}
 1&0&0\\0&0&0
 \end{bmatrix},
 \begin{bmatrix}
 0&1&0\\0&0&0
 \end{bmatrix},
 \begin{bmatrix}
 0&0&1\\0&0&0
 \end{bmatrix}\]
 \[
 \begin{bmatrix}
 0&0&0\\1&0&0
 \end{bmatrix},
 \begin{bmatrix}
 0&0&0\\0&1&0
 \end{bmatrix},
 \begin{bmatrix}
 0&0&0\\0&0&1
 \end{bmatrix}\]
\end{enumerate}
\end{ex}

\end{document}